%
% File naaclhlt2016.tex
%

\documentclass[11pt,letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage{naaclhlt2016}
\usepackage{times}
\usepackage{latexsym}
\usepackage{relsize}

\usepackage{verbatim}
\usepackage{booktabs}

\usepackage{tikz-qtree}
\usepackage{tikz-dependency}

\usepackage{xytree} % Dependency tree
\usepackage{fancyvrb} % Comments verbatim
\usepackage{fixltx2e} % For subscript i tekst (F_1 measure)
\usepackage{enumitem} % Kompakt liste


\naaclfinalcopy % Uncomment this line for the final submission
\def\naaclpaperid{***} %  Enter the naacl Paper ID here

\newcommand{\tov}{threat of violence}
\newcommand{\Tov}{Threat of violence}
\newcommand{\tovs}{threats of violence}
\newcommand{\Tovs}{Threats of violence}
\newcommand{\hugo}{Hugo Lewi Hammer}
\newcommand{\Dep}{Dependency parsing}
\newcommand{\dep}{dependency parsing}


\newcommand{\Ds}{The YouTube threat data set}
\newcommand{\ds}{the YouTube threat data set}

\let\tag=\textsc


% To expand the titlebox for more authors, uncomment
% below and set accordingly.
% \addtolength\titlebox{.5in}    

%todo: possibly a title that reflects the work better? 

% \title{Features for detecting threats of violence in YouTube comments}
\title{Threat detection in online discussions}
%\title{Online threat detection}
%\title{Threat detection for online comments}
%\title{Detecting threats of violence in online comments}
%\title{Detecting violent threats online}


% Author information can be set in various styles:
% For several authors from the same institution:
\author{Aksel Wester \and Lilja Øvrelid \and Erik Velldal \\
  Department of Informatics \\
  University of Oslo, Norway \\
  \{aksellw, liljao, erikve\}@ifi.uio.no}
% if the names do not fit well on one line use

%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}
% If the title and author information does not fit in the area allocated,
% place \setlength\titlebox{<new height>} right after
% at the top, where <new height> can be something larger than 2.25in
%% \author{Aksel Wester\\
%%   University of Oslo\\
%%   111 Anywhere Street\\
%%   Mytown, NY 10000, USA\\
%%   {\tt aksellw@ifi.uio.no}
%%   \And
%%   Lilja Øvrelid\\
%%   University of Oslo\\
%%   900 Main Street\\
%%   Ourcity, PQ, Canada A1A 1T2\\
%%   {\tt liljao@ifi.uio.no}
%%   \And
%%   Erik Velldal\\
%%   University of Oslo\\
%%   900 Main Street\\
%%   Ourcity, PQ, Canada A1A 1T2\\
%%   {\tt erikve@ifi.uio.no}}

\date{}

\begin{document}

\maketitle

\begin{abstract}
This paper investigates the effect of various types of linguistic features (lexical, syntactic and semantic) for training classifiers to detect threats of violence in a corpus of YouTube comments. % It further examines the use of these features in combination with different representations of linguistic context, and feature backoff. 
Our results show that combinations of lexical features outperform the use of more complex syntactic and semantic features for this task.
\end{abstract}

\section{Introduction}
\label{sec:intro}
Threats of violence constitute an increasingly common occurrence in online discussions. It disproportionately affects women and minorities, often to the point of effectively eliminating them from taking part in discussions online. Moderators of social networks operate on such a large scale that manually reading all posts is an insurmountable task. Methods for automatically detecting threats could therefore potentially be very helpful, both to moderators of social networks and to their members.

In this article, we evaluate different types of features for the task of detecting threats of violence in YouTube comments. We draw on both lexical, morphosyntactic and lexical semantic information sources and experiment with different machine learning algorithms. Our results indicate that successful detection of threats of violence is largely determined by lexical information.

\section{Related work}
\label{sec:prev}
There is little previous work specifically devoted to the detection of threats of violence in text. However, there is previous work which examines other types of closely related phenomena, such as `cyberbullying' and hate-speech.

\newcite{dinakar2011} propose a method for the detection of cyberbullying by targeting combinations of profane or negative words, and words related to several predetermined sensitive topics. Their data set consists of over 50,000 YouTube comments taken from videos about controversial topics. %% They adopt a two-stage approach, where the
%% first stage consists of using a lexicon of negative words and a list
%% of profane words, as well as part-of-speech tags from the training
%% data that were correlated with bullying. The second stage is
%% category-specific, and makes use of commonly observed uni- and bigrams
%% from each category as features.
The experiments reported accuracies from 0.63 to 0.80, but did not report precision or recall.
% lists of negative and profane words
% correlated words from training data

There has been quite a bit of work focused on the detection of threats in a data set of Dutch tweets \cite{nellngram2013,nellshallow2013}, which consists of a collection of 5000 threatening tweets. % collected by a website. 
In addition, a large number of random tweets were collected for development and testing. %% A set of 2.3 million random tweets was used
%% for development, and a set of 1 million was used for testing.
The system relies on manually constructed recognition patterns in the form of $n$-grams, 
%(uni-, bi- and trigrams, as well as skip bi- and trigrams)
but details about the strategy used to construct these patterns are not given. %% , stating that the researchers relied on their
%% (linguistic) intuition as speakers of Dutch \cite{nellngram2013}
In \newcite{nellshallow2013}, a manually crafted shallow parser is added to the system. This improves results to a precision of 0.39 and a recall of 0.59.
% handcrafted patterns
% rule-based shallow parser
% dubious evaluation

\newcite{warner2012} present a method for detecting hate speech in user-generated web text, which relies on machine learning in combination with template-based features. The task is approached as a word-sense disambiguation task, since the same words can be used in both hateful and non-hateful contexts. The features used in the classification were combinations of uni-, bi- and trigrams, part-of-speech-tags and Brown clusters.  The best results were obtained using only unigram features, with a precision of 0.67 and a recall of 0.60. The other feature sets garnered much lower results, and the authors suggest that deeper parsing could reveal significant phrase patterns.
% only unigrams best results
% suggests parsing might help

Most closely related to the current paper is the work of \newcite{hammer2014}, reporting on experiments on (a previous version of) the same corpus as used here. The method uses a logistic LASSO regression analysis on bigrams (skip-grams) of important words to classify sentences as either \tovs{} or not.  The system makes use of a list of words that are correlated with threats of violence. The article does not, however, describe exactly how these important words were selected, stating only that words were chosen that were significantly correlated with the response (violent/non-violent sentence). Results are reported only in terms of proportion of false positives for the two classes, and it is not clear how the data was split for training and evaluation, making it difficult to directly compare our results to those of \newcite{hammer2014}.


\section{The YouTube threat corpus}
\label{sec:data}
The YouTube threat corpus\footnote{Please contact the authors if you want to obtain access to the corpus for your own research.} comprises user-written comments from eight different YouTube videos \cite{hammer2014}. The videos cover religious and political topics like halal slaughter, immigration, Anders Behring Breivik, Jihad, etc. 
%The videos cover slightly different topics, the videos all contain the same
%type of discussions, namely of xenophobia and racism. 
A given comment consists of a set of sentences, each of them manually annotated to be either a \tov{} (or support for a \tov{}), or not. The corpus also records usernames of the commenters.
%% \footnote{In 2013, YouTube changed its
%%   commenting system from using unique usernames, to using "real
%%   names", like Facebook and other sites \cite{youtube2013}. Some
%%   accounts, however, did not provide real names, so they continue to
%%   only be identified by their usernames.}

As shown in Table~\ref{tab:dataset}, the corpus consists of 9,845 comments, comprising 28,643 sentences.
%% In total there are 402,673 tokens in the sentences in the data set.
There are 1,384 sentences containing threats, spread over 1,285 comments. \newcite{hammer2014} reports inter-annotator agreement on this data set to be 98~\%, as calculated on 120 of the comments, doubly annotated for evaluation.

\begin{table}
  \begin{smaller}
    \begin{center}
      \begin{tabular}{lrrr}
        % & & & \\
        \toprule
  
        & Comments  & Sentences  & Users posting \\
        \midrule
	
  
        Total  & $9,845$  & $28,643$  & $5,483$  \\
        Threats  & $1,285$  & $1,384$  & $992$  \\
        \bottomrule

      \end{tabular}
      \caption{Number of comments, sentences and users in \ds{}.}
      \label{tab:dataset}
    \end{center}
  \end{smaller}
\end{table}

%to-do: should we anonymize the user names?
Figure~\ref{fig:comments} provides some examples of comments from the corpus. The first line is the anonymized username, and the subsequent lines are the sentences of the comment. %% An empty line indicates the end of a comment.
The sentences are annotated with a number indicating whether they contain a \tov{} (1) or not (0).

\begin{figure}
\begin{Verbatim}[fontsize=\small]
User #44
1    and i will kill every fucking
     muslim and arab!

User #88
0    Need a solution?
1    Drop one good ol' nuke on that
     black toilet in Mecca.
\end{Verbatim}
\caption{Example comments from the YouTube threat corpus.}
\label{fig:comments}
\end{figure}

\section{Experiments}
\label{sec:exp}
Much of the previous related work presented in Section~\ref{sec:prev} made use of pre-compiled lists of correlated words and manually crafted patterns. Whereas these resources can be effective, they are highly task-specific and do not easily lend themselves to replication. Furthermore, while much of the previous work seem to highlight the effectiveness of lexical features, several of the authors also suggest that parsing may be beneficial for these tasks.

The approach followed in the current paper is to train a machine-learned model to automatically detect threats. We experiment with a range of different sources of linguistic information for defining our feature templates. 
We also generalize these features through a `backoff' technique. Throughout, we make use of resources and tools that are freely available and reusable.


\subsection{Experimental setup}

\paragraph{Pre-processing}Since threat annotation is performed on the sentence level, the corpus has been manually split into sentences as part of the annotation process.  We performed tokenization, lemmatization, POS-tagging and dependency parsing using the spaCy NLP toolkit. SpaCy assigns both the standard Penn Treebank POS-tags \cite{Mar:San:Mar:93}, as well as the more coarse-grained Universal POS tag set of \newcite{Pet:Das:McD:12}. The dependency parser assigns an analysis in compliance with the ClearNLP converter \cite{Cho:Pal:12}, see Figure~\ref{fig:dep} for an example dependency graph from the corpus. The corpus was further enriched with the cluster labels described in Turian et al.~\shortcite{Tur:Rat:Ben:10}, created using the Brown clustering algorithm \cite{Bro:deS:Mer:92} and induced from the Reuters corpus of English newswire text (RCV1).
%, with approximately 63 million words and 3.3 million sentences.
We vary the number of clusters to be either 100, 320, 1000 or 3200 clusters and use the full cluster label.  We also make use of the WordNet resource \cite{wordnet} to include information about the synset of a word, as well as its parent and grandparent synsets.

%% \begin{figure*}
%% \begin{center}
%% \begin{dependency}
%% %\begin{dependency}[arc edge, text only label, theme=simple]
%%   \begin{deptext}[column sep=.2cm]
%%     i \& m \& fucking \& scared \& kill \& them \& d \& : \\
%%    \tag{\smaller pron} \& \tag{\smaller verb} \& \tag{\smaller verb} \& \tag{\smaller adj} \& \tag{\smaller verb} \& \tag{\smaller pron} \& \tag{\smaller x} \& \tag{\smaller punct} \\
%%   \end{deptext}
%%   \depedge{2}{1}{\large nsubj}
%%   \depedge[edge unit distance=2ex]{2}{8}{\large punct}
%%   \depedge{2}{4}{\large acomp}
%%   \depedge{4}{3}{\large amod}
%%   \depedge{2}{5}{\large conj}
%%   \depedge{5}{6}{\large dobj}
%%   \depedge{5}{7}{\large advmod}
%%  \deproot{2}{root}
%% \end{dependency}
%% \caption{Dependency parse of example sentence from the data set, with assigned uPOS tags.}
%% \label{fig:dep}
%% \end{center}
%% \end{figure*}

\begin{figure}
\begin{center}
\begin{dependency}
%\begin{dependency}[arc edge, text only label, theme=simple]
  \begin{deptext}[column sep=.2cm]
    i \& m \& fucking \& scared \& kill \& them \& d\\
   \tag{\smaller pron} \& \tag{\smaller verb} \& \tag{\smaller verb} \& \tag{\smaller adj} \& \tag{\smaller verb} \& \tag{\smaller pron} \& \tag{\smaller x} \\
  \end{deptext}
  \depedge{2}{1}{\large nsubj}
  %% \depedge[edge unit distance=2ex]{2}{8}{\large punct}
  \depedge{2}{4}{\large acomp}
  \depedge{4}{3}{\large amod}
  \depedge{2}{5}{\large conj}
  \depedge{5}{6}{\large dobj}
  \depedge{5}{7}{\large advmod}
 \deproot{2}{root}
\end{dependency}
\caption{Dependency parse of example sentence from the corpus, with assigned uPOS tags.}
\label{fig:dep}
\end{center}
\end{figure}



%% \begin{figure*}
%%   \begin{center}

%%     \xytext{
%%       \xybarnode{i} &~~~&
%%       \xybarnode{m}
%%       \xybarconnect(UL,U){-2}"_{\small nsubj}"
%%       \xybarconnect[21](UR,U){12}"^{\small punct}"
%%       \xybarconnect[15](UR,UL){6}"^{\small conj}"
%%       \xybarconnect[9](UR,U){4}"^{\small acomp}"
%%       &~~~&
%%       \xybarnode{fucking} &~~~&
%%       \xybarnode{scared}
%%       \xybarconnect(UL,U){-2}"_{\small amod}"
%%       &~~~&
%%       \xybarnode{kill}
%%       \xybarconnect[9](U,U){4}"^{\small advmod}"
%%       \xybarconnect(UR,U){2}"^{\small dobj}"
%%       &~~~&
%%       \xybarnode{them}
%%       &~~~&
%%       \xybarnode{d}
%%       &~~~&
%%       \xybarnode{:}
%%     }
%%     \caption{A visual representation of the dependency graph of our example sentence.}
%%     \label{fig:dependencygraph}
%%   \end{center}
%% \end{figure*}

\paragraph{Classifiers}

We test three different classification frameworks in our development testing: Maximum Entropy (MaxEnt), Support Vector Machines (SVM), and Random Forests (RF).  We approach the task as a binary classification task, using the implementations found in the scikit-learn toolkit \cite{scikit-learn}.

\paragraph{Tuning}
When tuning each model, we aim to maximize the F-score. For the MaxEnt and SVM classifiers we tune the regularization parameter (C), where a smaller value corresponds to stronger regularization. When tuning these classifiers, we start with C-values from 1 to 150 in 10-value increments, select the best performing C-value and repeat the process with decreasing increments, with the range of C-values centered on the best performing C-value thus far. After 6 iterations, we terminate the tuning, and select the best performing C-value. When tuning the Random Forest classifier, we perform a grid search over the maximum number of features used when splitting a node in the tree (sqrt($n$), log($n$) or $n$, where $n$ is the number of features in the model), and the number of trees (10, 100 or 1000, depending on the size of the feature set). In the following experiments, we perform tuning on all feature sets.

% TODO: Ta med siste setning, om at vi alltid tuner
%% When performing tuning on the MaxEnt classifier with the bag of words features set, F-score improved from $0.5622$, using the default settings (C-value=1), to $0.6123$, using the best C-value ($11.25$). The large increase in F-score after tuning leads us to perform tuning on every feature set we test.

\paragraph{Features}
Based on the enriched corpus, as described above, we experiment with the following sources of information for defining our features:

\begin{itemize}[noitemsep]
\item Lexical:
  \begin{itemize}[noitemsep]
  \item Word form
  \item Lemma
  \end{itemize}
  
\item Morphosyntactic:
  \begin{itemize}[noitemsep]
  \item Penn Treebank (PTB) POS
  \item Universal POS (uPOS)
  \item Dependency Relation
  \end{itemize}
  
\item Semantic:
  \begin{itemize}[noitemsep]
    \item Brown cluster label %(100, 320, 1000 and 3200 clusters)
    \item WordNet synset, + parent and grandparent
  \end{itemize}
\end{itemize}

%% \begin{description}
%% \setlength\itemsep{0em}
%% \item[Lexical] word form, lemma
%% \item[Morphosyntactic] Penn Treebank (PTB) or Universal (uPOS) PoS and Dependency Relation
%% \item[Semantic] Brown cluster label (100, 320, 1000 and 3200 clusters) and WordNet synset, parent synset and grandparent synset
%% \end{description}
The features are structured according to a set of \emph{feature templates},
which record varying degrees of linear order and syntactic context:
\emph{bag-of} features (unordered), bigrams, trigrams and dependency
triples. Examples of the latter, given the sentence in Figure~\ref{fig:dep},
would be: \{$<$m, nsubj, i$>$, $<$root, root, m$>$, $<$scared, amod, fucking$>$, \ldots \}

%These feature templates are applied with the information sources presented above, yielding features like the following for our examples sentence in Figure~\ref{fig:dep}:
%% \begin{itemize}
%% \setlength\itemsep{0em}
%% \item Bigrams of word forms: \{i m,  m fucking, fucking scared, ...\}
%%%
%% \item Dependency triples: \{$<$m, nsubj, i$>$, $<$root, root, m$>$, $<$scared, amod, fucking$>$, ... \}
%% \end{itemize}

Our lexicalized features are very specific and require the exact combination of two lexical items in order to apply to a new instance. Following \newcite{Jos:Pen:2009}, we therefore experiment with generalizing features by `backing off' to a more general category, e.g., from word form to lemma or POS. For example, a dependency triple over word forms like $<$kill, dobj, them$>$ would thus be generalized to $<$VERB, dobj, them$>$ using \emph{head-backoff}, and $<$kill, dobj, PRON$>$ using \emph{modifier-backoff}. These additional backoff features are included for bigrams and trigrams as well as dependency triples.

We impose a simple count-based reduction of the feature set; only features appearing at least twice in the training data are included in the model.


\subsection{Development results}

We start by defining an informed baseline system, empirically selecting an initial set of features and a classification framework to use as a basis and reference point for further development. The features for this initial round of tuning comprise basic lexical features; word forms and lemmas, in addition to $n$-grams defined over these.

In the second round of experiments we test combinations of these basic lexical feature types, before moving on to introduce more linguistic features, both syntactic and semantic, as \emph{bag-of} features, and as backoff from word form $n$-grams. Finally, we will evaluate the inclusion of dependency triples. %, using both word form triples, and dependency triples with feature backoff.

\begin{table}
  \begin{smaller}
    \begin{center}
      \begin{tabular}{lrrr}
        \toprule

        & MaxEnt & SVM & RF \\
        \midrule
        Bag-of-words & $\textbf{0.6123}$ & $\textbf{0.6068}$ & $\textbf{0.5918}$ \\ % SVM bag of words er bare best dersom word form + uPOS ikke er med i tabellen
        Bag-of-lemmas & $0.5902$ & $0.5982$ & $0.5856$ \\
        Bigrams of word forms & $0.4856$ & $0.4887$ & $0.4944$ \\
        Trigrams of word forms & $0.2776$ & $0.2856$ & $0.2859$ \\      
      
        \bottomrule
        % Data fra firstPass i classify
        % Source/printFirstPassResults.py
      \end{tabular}
    \end{center}
    \caption{Results for baseline system; F-score for bag-of lexical features
      (word form and lemma), and bigram and trigram templates over word
      forms. From left to right the columns correspond to Maximum Entropy
      classifiers, Support Vector Machines, and Random Forests.}
    \label{tab:baseline}
  \end{smaller}
\end{table}

Table~\ref{tab:baseline} shows initial development results in terms of F-score for the the three different classifiers across four different feature sets; bags of word-forms and lemmas, as well as bigrams and trigrams over word forms. Generally, we see that feature sets containing lexical \emph{bag-of} features outperform the $n$-gram features. The overall best result came from the MaxEnt classifier with the bag-of-word form feature set. This model yielded an F-score of $0.6123$, with a precision of $0.6777$ and a recall of $0.5585$, using the MaxEnt classifier after tuning. This is the feature set we will use as our basic reference model in the next stage of our experiments is the bag-of-word forms. We will also only be using the the MaxEnt classifier for the remainder for reported development experiments. Not only did the MaxEnt model have the best performance for the basic BoW feature sets, but it is also the framework that is most convenient to use for exploring many combinations of features given that a MaxEnt model can be trained in much less time than an SVM or Random Forest classifier.

\begin{table}
  \begin{smaller}
    \begin{center}
      \begin{tabular}{lrrr}
        \toprule
      
        $n$-gram combination & BoW & BoW+BoL \\
        \midrule
        no $n$-grams & $0.6123$ & $0.6278$ \\
        +bigrams & $0.6376$ & $0.6577$ \\
        +trigrams & $0.6180$ & $0.6453$ \\
        +bi- and trigrams & $0.6337$ & $\textbf{0.6656}$ \\
        \bottomrule

      \end{tabular}
    \end{center}
    \caption{F-scores of the bag-of-words feature set, with different
      combinations of the other feature sets tested in Table~\ref{tab:baseline}, namely bag-of-lemmas and $n$-grams of word forms.}
    \label{tab:ngrams}
    % tidligere Tabell 4
  \end{smaller}
\end{table}

\begin{table}
  \begin{smaller}    
  \begin{center}
    \begin{tabular}{lrrrrr}
      \toprule
      
     & POS\textsubscript{Lex} & Dep\textsubscript{Lex} & Synset & Brown\\ % POS er uPOS
      \midrule
      BoF & $0.6018$ & $0.5655$ & $0.4922$ & $0.4688$ \\ % Synset regular, Brown 3200
      %% BoF & $0.0018$  &  $0.0143$ & $0.4922$ & $0.4688$ \\ % Synset regular, Brown 3200
      BoW+BoF & $0.6071$ & $0.6185$ & $0.6176$ & $0.6145$\\ % Synset regular, Brown100
      
      \bottomrule

    \end{tabular}
  \end{center}
  \caption{F-scores of \emph{bag-of} features (BoF) with and without bag-of-words (BoW). POS\textsubscript{Lex} and Dep\textsubscript{Lex} are lexicalized POS and dep-tags, respectively, where each feature is a tuple consisting of the tag and the word form of the token.}
  \label{tab:bagOfFeatures}
  % tidligere Tabell 3
  \end{smaller}
\end{table}

We go on to test various combinations of these word form $n$-grams and the lexical \emph{bag-of} features. As seen in Table~\ref{tab:ngrams}, we test bag-of-words alone, and bag-of-words (BoW) with bag-of-lemmas (BoL), combined with the word form variants of bigrams, trigrams and both. The best result without BoL comes from BoW with only bigrams, with an F-score of $0.6376$, closely followed by BoW with both types of $n$-grams, which got an F-score of $0.6337$. However, the best result overall came from the combination of BoW, BoL, and both types of $n$-grams, which got an F-score of $0.6656$.

Next, we include feature types based on the other information sources listed above, i.e., POS, dependencies, WordNet synsets, and Brown clusters. All these features are instantiated both with the \emph{bag-of} feature template on their own, and in combination with the bag-of-words features. As seen in Table~\ref{tab:bagOfFeatures}, none of the feature types alone yield higher F-scores than bag-of-words. On the other hand, when combined with BoW, all feature types (except the lexicalized POS-tags) perform better than BoW alone. However, none of them outperform BoW and BoL combined (F=$0.6278$, cf. Table~\ref{tab:ngrams}), or the combination of Bow+BoL with $n$-grams (F=$0.6656$).

\begin{table}
  \begin{smaller}    
  \begin{center}
    \begin{tabular}{lrrrrrr}
      \toprule
      %% BoW, BoF \& 
      %% & Lemma & POS & Dep & Synset & Brown\\
      %% \midrule
      %% Bigram + backoff
      %%  \\ % synset reg, b3200
      %% Trigram + backoff
      %%  \\ % synset reg, b320
      %% $n$-gram + backoff 
      %% \\ % synset reg, b320
      & bigram & trigram & bi+trigram \\
      \midrule
      Lemma backoff & $0.6611$ & $0.6480$ &  $\textbf{0.6649}$ \\ 
      POS backoff & $0.6410$ & $0.6294$ & $0.6320$ \\
      Dep backoff & $0.6208$ & $0.6194$ & $0.6220$ \\
      Synset backoff & $0.6454$ & $0.6448$ &  $0.6537$\\
      Brown backoff & $0.6285$ &  $0.6173$  & $0.6335$\\
      
      %Bigram + backoff, trigram & $\textbf{0.6649}$ & $0.6398$ & $0.6309$ & $0.6192$ \\
      %Bigram, trigram + backoff & $\textbf{0.6649}$ & $0.6320$ & $\textbf{0.6368}$ & $\textbf{0.6220}$ \\
      %Bigram + backoff, trigram + backoff & $0.6645$ & $0.6316$ & $0.6268$ & $0.6127$ \\
      
      \bottomrule

    \end{tabular}
  \end{center}
  \caption{Backoff from different combinations of $n$-grams. The models also
    contain bag-of-words and the other \emph{bag-of} features. Each backoff combination is the one that achieved the best result.}
  \label{tab:ngrambackoff}
  \end{smaller}
\end{table}

Next, the $n$-grams and \emph{bag-of} features are combined with generalized versions of $n$-gram features using the `backoff' strategy described in the previous section. In Table~\ref{tab:ngrambackoff} we see that the lemma backoff consistently outperform the other feature types, but that even lemma backoff does not improve upon the results without backoff features. We test all possible backoff combinations.
% fixme: forklar mer eksplisitt hvilke trekk som inngår i modellene i tabell 5, jeg klarte ikke gjøre rede for det selv.

\begin{table}
  \begin{smaller}    
  \begin{center}
    \begin{tabular}{lrr}
      \toprule           
      Dependency backoff & BoW+dep & All\\
      \midrule
      w/o backoff & $0.6240$ & $\textbf{0.6586}$ \\
      Lemma & $0.6224$ & $0.6507$ \\
      POS & $\textbf{0.6298}$ & $0.6547$ \\ % uPOS, begge to
      Synset & $0.6234$ & $0.6516$  \\ % 1. regular og granparent (de er like), 2. grand parent
      Brown & $0.6299$ & $0.6504$ \\ % 1. Brown 320, 2. Brown 1000                 
      \bottomrule
    \end{tabular}
  \end{center}
  \caption{Dependency triples with and without feature backoff, in combination with other features: 
`BoW+dep' is the feature set containing bag-of-words and dependency triples. `All' is the feature set containing BoW, BoL, bi- and trigrams and dependency triples. Each row backs off to a different feature type.}
  \label{tab:depbackoff}
  \end{smaller}
\end{table}

Lastly, we will test the addition of dependency triples to our models. We add
dependency triples consisting of word forms, both alone, and in conjunction
with feature backoffs. As with $n$-gram backoff, we test multiple variations of
backoff, both head-backoff and modifier-backoff. The results reported are the
backoff variant that achieved the best results. As seen in Table
\ref{tab:depbackoff}, the inclusion of word form dependency triples improved
upon the simplest model, with an F-score of $0.6240$, compared to $0.6123$ for
bag-of-words alone. Dependency triples did not, however, improve upon the
results achieved by our previous best performer with an F-score of $0.6656$. %, as the best model with dependency triples got an F-score of $0.6586$ compared to our best result of $0.6656$.
Neither did the addition of backoff improve upon this. Adding POS backoff
to the `BoW+dep' configuration resulted in a slightly increased F-score of
$0.6298$. However, adding backoff to the `All' feature set only resulted in poorer F-scores, with the best feature backoff (also POS) resulting in an F-score of $0.6547$.

For the development data then, our best performer remains the feature set comprising bag-of-word forms, bag-of-lemmas, and word form bigrams and trigrams; with an F-score of $0.6656$. We will refer to this model as `lexical $n$-grams'. % fixme: burde kanskje ha introdusert denne termen tidligere?

 % In Section~\ref{sec:heldout} we will also test that same model with the addition of trigram backoff to lemmas, which got an F-score of $0.6649$.

%% Morphosyntactic features:
%% - bag of, bigram, trigram of pos, deprel
%% - combine with baseline

%% Semantic features:
%% - bag of, bigram, trigram of Brown, WordNet
%% - combine with baseline

%% Backoff: best of morphosyntactic and semantic?

%% Combination of best from baseline, morphosyntactic and semantic

%\subsection{Error analysis}

%% Before we perform our held-out testing, we will examine what types of errors we have in our model, when tested on the development set. 
The lexical $n$-gram feature set achieved a precision (P) of $0.7709$ and a recall (R) of $0.5857$. Compared with our initial BoW system, which had P=$0.6777$ and R=$0.5585$, we see that the majority of the improvement comes from the increase in precision. When reviewing a random sub-sample of the false positives and false negatives, we see that the noisy data has caused some problems for the pre-processor. % fixme: kanskje forklare mer hva det siktes til her?
Another source of errors, specifically for precision, are comments which use multiple typically threatening words in a non-threatening context.

% fixme: gi eksempler på fenomener vi sikter til her?

%% \begin{table}
%%   \begin{center}
%%     \begin{tabular}{llrr}
%%       \toprule
      
%%       & & Actual & \\
%%       &  & Positive & Negative\\
%%       \midrule
%%       Predicted & Positive & $646$ & $192$ \\
%%       & Negative & $457$ & $21,663$ \\
      
      
%%       \bottomrule

%%     \end{tabular}
%%   \end{center}
%%   \caption{The true and false, positives and negatives of our best model, when classifying the development set. The numbers are the sum of each of the 10 classification, when performing 10-fold cross-validation.}
%%   \label{tab:positives}
%% \end{table}

\subsection{Held-out results}
\label{sec:heldout}

We performed our final testing on the held-out test set using the basic BoW model and the lexical $n$-gram model. As seen in Table~\ref{tab:heldout}, the $n$-gram model outperforms the BoW model by a good margin, with F-scores of $0.6860$ and $0.6562$ respectively. However, the difference in F-scores between the two feature sets is not as large as on the development data. At the same time, we see that both models actually achieves higher scores on the held-out data than the development data, and this effect is particularly strong for the BoW model (with the F-score going up from $0.6123$ to $0.6562$). % fixme: noen teori om hvorfor vi ser denne effekten?
% fixme: si noe som størrelsen på held-out dataene.

% fixme: sier vi noe sted eksplisitt om testing skjer på setningsnivå eller kommentarnivå?

% However, this may be due to the fact that the basic BoW model performed a lot better in the held-out testing compared to the development testing, with an F-score of $0.6562$. The F-scores of the combined feature sets were $0.6860$ and $0.6799$. In fact, both the combined feature sets improved upon the baseline in both precision, recall and F-scores.

\begin{table}
\begin{smaller}
  \begin{center}
    \begin{tabular}{lrrr}
      \toprule      
      & Precision & Recall & F-score\\
      \midrule
      BoW & $0.7325$ & $0.5943$ & $0.6562$ \\
      Lexical $n$-grams & $\textbf{0.7532}$ & $\textbf{0.6299}$ & $\textbf{0.6860}$  \\
%      Combined+backoff & $\textbf{0.7703}$ & $0.6085$ & $0.6799$ \\            
      \bottomrule
    \end{tabular}
  \end{center}
  \caption{Precision, recall and F-score on the held-out test set for the basic BoW model and the best development feature set; the lexical $n$-gram model.}
  \label{tab:heldout}
\end{smaller}
\end{table}


\section{Conclusion}
\label{sec:conclusion}

This paper has developed and compared several machine-learned models for automatically detecting threats of violence in online discussions. The data set comprises a manually annotated corpus of comments from YouTube videos. We have reported experimental results for different classification frameworks and a wide range of different linguistic features. The best performance was observed for combinations of simple lexical features (bag-of-words- and lemmas, in combination with bi- and trigrams). Introducing more complex features -- drawing on information from WordNet synsets, Brown clusters, POS tags and dependency parses -- did not improve on the simpler surface-based feature set.


\bibliography{wassa2016}
\bibliographystyle{naaclhlt2016}


\end{document}
