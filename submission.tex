%
% File naaclhlt2016.tex
%

\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2016}
\usepackage{times}
\usepackage{latexsym}
\usepackage{relsize}

\usepackage{verbatim}
\usepackage{booktabs}

\usepackage{tikz-qtree}
\usepackage{tikz-dependency}

\usepackage{xytree} % Dependency tree
\usepackage{fancyvrb} % Comments verbatim
\usepackage{fixltx2e} % For subscript i tekst (F_1 measure)
\usepackage{enumitem} % Kompakt liste


%\naaclfinalcopy % Uncomment this line for the final submission
\def\naaclpaperid{***} %  Enter the naacl Paper ID here

\newcommand{\tov}{threat of violence}
\newcommand{\Tov}{Threat of violence}
\newcommand{\tovs}{threats of violence}
\newcommand{\Tovs}{Threats of violence}
\newcommand{\hugo}{Hugo Lewi Hammer}
\newcommand{\Dep}{Dependency parsing}
\newcommand{\dep}{dependency parsing}


\newcommand{\Ds}{The YouTube threat data set}
\newcommand{\ds}{the YouTube threat data set}

\let\tag=\textsc


% To expand the titlebox for more authors, uncomment
% below and set accordingly.
% \addtolength\titlebox{.5in}    

%todo: possibly a title that reflects the work better? 

\title{Features for detecting threats of violence in YouTube comments}
%\title{Detecting threats of violence in YouTube comments}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use

%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}
% If the title and author information does not fit in the area allocated,
% place \setlength\titlebox{<new height>} right after
% at the top, where <new height> can be something larger than 2.25in
\author{Author 1\\
	    XYZ Company\\
	    111 Anywhere Street\\
	    Mytown, NY 10000, USA\\
	    {\tt author1@xyz.org}
	  \And
	Author 2\\
  	ABC University\\
  	900 Main Street\\
  	Ourcity, PQ, Canada A1A 1T2\\
  {\tt author2@abc.ca}}

\date{}

\begin{document}

\maketitle

\begin{abstract}
This article investigates the effect of various types of linguistic
features (lexical, morphosyntactic and lexical semantic) for the task of
detecting threats of violence in a data set of YouTube comments. It
further examines the use of these features in combination with
different representations of linguistic context, and feature backoff. Our results show that combinations of lexical features outperform the use of more complex syntactic and semantic features for this task.
\end{abstract}

\section{Introduction}
\label{sec:intro}
Threats of violence constitute an increasingly common occurrence in online
discussions. It disproportionally affects women and minorities, often
to the point of effectively eliminating them from taking part in
discussions online. Moderators of social networks operate on such a
large scale that manually reading all posts is an insurmountable
task. Methods for automatically detecting threats could therefore
potentially be very helpful, both to moderators of social networks,
and to the members of those networks.

In this article, we evaluate different types of features for the task
of detecting threats of violence in YouTube comments. We draw on both
lexical, morphosyntactic and lexical semantic information sources and
experiment with different machine learning algorithms. Our results
indicate that successful detection of threats of violence is largely
determined by lexical information.

\section{Previous work}
\label{sec:prev}
There is little previous work specifically devoted to the detection of
threats of violence in text, however, there is previous work which
examines other types of closely related phenomena, such as
cyberbullying and hate-speech.

\newcite{dinakar2011} propose a method for the detection of
cyberbullying by targeting combinations of profane or negative words,
and words related to several predetermined sensitive topics. Their
data set consists of over 50,000 YouTube comments taken from videos
about controversial topics. %% They adopt a two-stage approach, where the
%% first stage consists of using a lexicon of negative words and a list
%% of profane words, as well as part-of-speech tags from the training
%% data that were correlated with bullying. The second stage is
%% category-specific, and makes use of commonly observed uni- and bigrams
%% from each category as features.
The experiments reported accuracies from 0.63 to 0.80, but did not
report precision or recall.
% lists of negative and profane words
% correlated words from training data

There has been quite a bit of work focused on the detection of threats
in a data set of Dutch tweets \cite{nellngram2013,nellshallow2013}, which consists of a collection of 5000
threatening tweets collected by a website. In addition, a large number of random tweets were collected for
development and testing. %% A set of 2.3 million random tweets was used
%% for development, and a set of 1 million was used for testing.
The system relies on manually constructed recognition patterns, in the
form of n-grams 
%(uni-, bi- and trigrams, as well as skip bi- and trigrams)
, but do not go into detail about the methods used to
construct these patterns%% , stating that the researchers relied on their
%% (linguistic) intuition as speakers of Dutch \cite{nellngram2013}
. In
\newcite{nellshallow2013}, a manually crafted shallow parser is added
to the system. This improves results to a precision of 0.39 and a
recall of 0.59.
% handcrafted patterns
% rule-based shallow parser
% dubious evaluation

\newcite{warner2012} present a method for detecting hate speech in
user-generated web text from the internet, which relies on machine
learning in combination with template-based features. The task
is approached as a word-sense disambiguation task, since the same
words can be used in both hateful and non-hateful contexts. The
features used in the classification were combinations of uni-, bi- and
trigrams, part-of-speech-tags and Brown clusters.  The best results of
the classifications were obtained using only unigrams as features,
with a precision of 0.67 and a recall of 0.60. The other feature
sets garnered much lower results, and the authors suggest that deeper
parsing could reveal significant phrase patterns.
% only unigrams best results
% suggests parsing might help

Perhaps closest to the current work is the work of
\newcite{hammer2014}, which reports on an experiment on the same data set that
we will be using in our own experiments. The method uses a logistic
LASSO regression analysis on bigrams (skip-grams) of important words
to classify sentences as either \tovs{} or not.  The system makes use of a
list of words that are correlated with threats of violence. The
article does not, however, describe exactly how these important words
were selected, stating only that words were chosen that were
significantly correlated with the response (violent/non-violent
sentence). Results are reported only in terms of proportion of false
positives for the two classes, and it is not clear how the data was
split for training and evaluation. This makes it difficult to compare our results to those of \newcite{hammer2014}.


\section{The YouTube threat data set}
\label{sec:data}
The YouTube threat data set is comprised of user-written comments from
eight different YouTube videos \cite{hammer2014}. A comment
consists of a set of sentences, each of them manually annotated to be
either a \tov{} (or support for a \tov{}), or not. The data set
furthermore records the username
%% \footnote{In 2013, YouTube changed its
%%   commenting system from using unique usernames, to using "real
%%   names", like Facebook and other sites \cite{youtube2013}. Some
%%   accounts, however, did not provide real names, so they continue to
%%   only be identified by their usernames.}
  of the user that posted the
comment. The eight videos that the comments were posted to cover religious and political topics like halal slaughter, immigration,
Anders Behring Breivik, Jihad, etc. \cite{hammer2014}. 
%The videos cover slightly different topics, the videos all contain the same
%type of discussions, namely of xenophobia and racism.

The data set consists of 9,845 comments, comprised of 28,643
sentences, see Table \ref{tab:dataset}.
%% In total there are 402,673 tokens in the sentences in the data set.
There are 1,285 comments
containing threats, and 1,384 sentences containing threats, as seen in
Table \ref{tab:dataset}.  \newcite{hammer2014} reports inter-annotator
agreement on this data set to be 98~\%, as calculated on 120 of the
comments, doubly annotated for evaluation.

\begin{table}
\begin{center}
\begin{tabular}{lrrr}
  %&   &   &   \\
  \toprule
  
  & Comments  & Sentences  & Users posting \\
  \midrule
	
  
  Total  & $9,845$  & $28,643$  & $5,483$  \\
  Threats  & $1,285$  & $1,384$  & $992$  \\
  \bottomrule

\end{tabular}
\end{center}
\caption{Number of comments, sentences and users in \ds{}.}
\label{tab:dataset}
\end{table}

%to-do: should we anonymize the user names?
Figure \ref{fig:comments} provides some examples of comments containing
\tovs{} taken from the data set. The first line is the annonymized username, and the subsequent lines are the sentences of the comment.
%% An empty line indicates the end of a comment.
The sentences are annotated
with a number indicating whether they contain a \tov{} (1), or not
(0).

\begin{figure}
\begin{Verbatim}[fontsize=\small]
User #44
1    and i will kill every fucking
     muslim and arab!

User #88
0    Need a solution?
1    Drop one good ol' nuke on that
     black toilet in Mecca.
\end{Verbatim}
\caption{Examples of comments from the data set.}
\label{fig:comments}
\end{figure}

\section{Experiments}
\label{sec:exp}
Much of the previous work presented in Section \ref{sec:prev} made use of
precompiled lists of correlated words and manually crafted
patterns. Whereas these resources can be effective, they are highly
task-specific and do not easily lend themselves to replication.  The
previous work further highlights the effectiveness of lexical features
(word-based features), however, several of the authors suggest that parsing
may be beneficial for these tasks.

In this work we experiment with a wide range of linguistic features in our system and investigate the introduction of
linguistic context through a set of feature templates. We also
generalize these features through a backoff technique. Throughout, we
make use of resources and tools that are freely available and reusable in order
to construct our system.


\subsection{Experimental setup}

\paragraph{Pre-processing}Since threat annotation is performed on the sentence level, the data
set has been manually split into sentences as part of the annotation
process.  We performed tokenization, lemmatization, PoS-tagging and
dependency parsing using the SpaCy
toolkit\footnote{https://spacy.io/}.  SpaCy assigns both the standard
Penn Treebank PoS-tags \cite{Mar:San:Mar:93}, as well as the more
coarse-grained Universal PoS tag set of \newcite{Pet:Das:McD:12}. The
dependency parser assigns an analysis in compliance with the ClearNLP
converter \cite{Cho:Pal:12}, see Figure \ref{fig:dep} for an example
dependency graph from the data set. The data set was further enriched
with the cluster labels described in Turian et
al.~\shortcite{Tur:Rat:Ben:10}, created using the Brown clustering
algorithm \cite{Bro:deS:Mer:92} and induced from the RCV1 corpus, a
corpus containing Reuters English newswire text.
%, with approximately 63 million words and 3.3 million sentences.
We vary the number of
clusters to be either 100, 320, 1000 or 3200 clusters and use the full
cluster label.  We also make use of the WordNet resource
\cite{wordnet} to determine a word's synset, parent and grandparent
synset.

%% \begin{figure*}
%% \begin{center}
%% \begin{dependency}
%% %\begin{dependency}[arc edge, text only label, theme=simple]
%%   \begin{deptext}[column sep=.2cm]
%%     i \& m \& fucking \& scared \& kill \& them \& d \& : \\
%%    \tag{\smaller pron} \& \tag{\smaller verb} \& \tag{\smaller verb} \& \tag{\smaller adj} \& \tag{\smaller verb} \& \tag{\smaller pron} \& \tag{\smaller x} \& \tag{\smaller punct} \\
%%   \end{deptext}
%%   \depedge{2}{1}{\large nsubj}
%%   \depedge[edge unit distance=2ex]{2}{8}{\large punct}
%%   \depedge{2}{4}{\large acomp}
%%   \depedge{4}{3}{\large amod}
%%   \depedge{2}{5}{\large conj}
%%   \depedge{5}{6}{\large dobj}
%%   \depedge{5}{7}{\large advmod}
%%  \deproot{2}{root}
%% \end{dependency}
%% \caption{Dependency parse of example sentence from the data set, with assigned uPOS tags.}
%% \label{fig:dep}
%% \end{center}
%% \end{figure*}

\begin{figure}
\begin{center}
\begin{dependency}
%\begin{dependency}[arc edge, text only label, theme=simple]
  \begin{deptext}[column sep=.2cm]
    i \& m \& fucking \& scared \& kill \& them \& d\\
   \tag{\smaller pron} \& \tag{\smaller verb} \& \tag{\smaller verb} \& \tag{\smaller adj} \& \tag{\smaller verb} \& \tag{\smaller pron} \& \tag{\smaller x} \\
  \end{deptext}
  \depedge{2}{1}{\large nsubj}
  %% \depedge[edge unit distance=2ex]{2}{8}{\large punct}
  \depedge{2}{4}{\large acomp}
  \depedge{4}{3}{\large amod}
  \depedge{2}{5}{\large conj}
  \depedge{5}{6}{\large dobj}
  \depedge{5}{7}{\large advmod}
 \deproot{2}{root}
\end{dependency}
\caption{Dependency parse of example sentence from the data set, with assigned uPOS tags.}
\label{fig:dep}
\end{center}
\end{figure}



%% \begin{figure*}
%%   \begin{center}

%%     \xytext{
%%       \xybarnode{i} &~~~&
%%       \xybarnode{m}
%%       \xybarconnect(UL,U){-2}"_{\small nsubj}"
%%       \xybarconnect[21](UR,U){12}"^{\small punct}"
%%       \xybarconnect[15](UR,UL){6}"^{\small conj}"
%%       \xybarconnect[9](UR,U){4}"^{\small acomp}"
%%       &~~~&
%%       \xybarnode{fucking} &~~~&
%%       \xybarnode{scared}
%%       \xybarconnect(UL,U){-2}"_{\small amod}"
%%       &~~~&
%%       \xybarnode{kill}
%%       \xybarconnect[9](U,U){4}"^{\small advmod}"
%%       \xybarconnect(UR,U){2}"^{\small dobj}"
%%       &~~~&
%%       \xybarnode{them}
%%       &~~~&
%%       \xybarnode{d}
%%       &~~~&
%%       \xybarnode{:}
%%     }
%%     \caption{A visual representation of the dependency graph of our example sentence.}
%%     \label{fig:dependencygraph}
%%   \end{center}
%% \end{figure*}

\paragraph{Classifiers}

We test three different classifiers in our development testing; a Maximum Entropy (MaxEnt) classifier, a Support Vector Machine (SVM), and a RandomForest classifier (RF).  We approach the task as a binary classification task, and we use scikit-learn \cite{scikit-learn} for our classification.

\paragraph{Tuning}
When tuning each model, we aim to maximize the F-score of the model. In the MaxEnt classifier and SVM we tune the C-value, which for both classifiers is the inverse of regularization strength. When tuning these classifiers, we start with C-values from 1 to 150 in 10-value increments, select the best performing C-value and repeat the process with ever-decreasing increments, with the range of C-values centered on the best performing C-value thus far. After 6 iterations, we terminate the tuning, and select the best performing C-value. When tuning the RandomForest classifier, we perform a grid search over the maximum number of features used when splitting a node in the tree (sqrt($n$), log($n$) or $n$, where $n$ is the number of features in the model), and the number of trees (10, 100 or 1000, depending on the size of the feature set). In the following experiments, we perform tuning on all feature sets.

% TODO: Ta med siste setning, om at vi alltid tuner
%% When performing tuning on the MaxEnt classifier with the bag of words features set, F-score improved from $0.5622$, using the default settings (C-value=1), to $0.6123$, using the best C-value ($11.25$). The large increase in F-score after tuning leads us to perform tuning on every feature set we test.

\paragraph{Features}
Based on the enriched data set, as described above, we experiment with the following feature types:

\begin{itemize}[noitemsep]
\item Lexical:
  \begin{itemize}[noitemsep]
  \item Word form
  \item Lemma
  \end{itemize}
  
\item Morphosyntactic:
  \begin{itemize}[noitemsep]
  \item Penn Treebank (PTB) POS
  \item Universal (uPOS) PoS
  \item Dependency Relation
  \end{itemize}
  
\item Semantic:
  \begin{itemize}[noitemsep]
    \item Brown cluster label (100, 320, 1000 and 3200 clusters)
    \item WordNet synset, parent synset and grandparent synset
  \end{itemize}
\end{itemize}


%% \begin{description}
%% \setlength\itemsep{0em}
%% \item[Lexical] word form, lemma
%% \item[Morphosyntactic] Penn Treebank (PTB) or Universal (uPOS) PoS and Dependency Relation
%% \item[Semantic] Brown cluster label (100, 320, 1000 and 3200 clusters) and WordNet synset, parent synset and grandparent synset
%% \end{description}
These features are structured according to a set of \emph{feature
  templates}, which introduce varying degrees of linear order and
syntactic context: bag-of-features (unordered), bigrams, trigrams and
dependency triples. These feature templates are applied to the feature
types presented above, in order to yield features like the following
for our examples sentence in Figure \ref{fig:dep}:

\begin{itemize}
\setlength\itemsep{0em}
\item Bigrams of word forms: \{i m,  m fucking, fucking scared, ...\}

\item Depdendency triples: \{$<$m, nsubj, i$>$, $<$root, root, m$>$, $<$scared, amod, fucking$>$, ... \}
\end{itemize}
Our lexical features are very specific and require the exact
combination of two lexical items in order to apply to a new instance.
Following \newcite{Jos:Pen:2009}, we therefore experiment with
generalization of features by backoff to a more general category,
e.g. from word form to lemma or PoS. A dependency triple over word
forms like $<$kill, dobj, them$>$ would thus be generalized to
$<$VERB, dobj, them$>$ using \emph{head-backoff}, and $<$kill, dobj,
PRON$>$ using \emph{modifier-backoff}. We apply backoff to bigram,
trigram and dependency triple features.



\subsection{Development results}

During development, we first select a baseline system, consisting of a feature set of lexical features (word forms and lemmas) and a classifier, that we will compare the other results to. Next, we will test different combinations of the lexical feature sets used in the baseline testing. We will then introduce more linguistic features, both morphosyntactic and lexical semantic, as bags of features, and as backoff from word form $n$-grams. Finally, we will evaluate the inclusion of dependency triples, using both word form triples, and dependency triples with feature backoff.

\begin{table}
  \begin{center}
    \begin{tabular}{lrrr}
      \toprule

      & MaxEnt & SVM & RF \\
      \midrule
      Bag of word forms & $\textbf{0.6123}$ & $\textbf{0.6068}$ & $\textbf{0.5918}$ \\ % SVM bag of words er bare best dersom word form + uPOS ikke er med i tabellen
      Bag of lemmas & $0.5902$ & $0.5982$ & $0.5856$ \\
      Bigrams & $0.4856$ & $0.4887$ & $0.4944$ \\
      Trigrams & $0.2776$ & $0.2856$ & $0.2859$ \\
      
      
      \bottomrule
      % Data fra firstPass i classify
      % Source/printFirstPassResults.py
    \end{tabular}
  \end{center}
  \caption{Results for baseline system; F-score for bag-of lexical features (word form and lemma), and bigram and trigram templates over word forms . MaxEnt is the MaximumEntropy classifier, SVM is the support vector machine, and RF is the RandomForest classifier.}
  \label{tab:baseline}
\end{table}

Table \ref{tab:baseline} shows baseline results in terms of F-score for bag-of lexical features (word form and lemma), and bigram and trigram templates over word form, for the three different classifiers. The overall best result came from the MaxEnt classifier with the bag of word form feature set. Generally, we see that feature sets containing bag of lexical features outperform the $n$-gram feature sets. The feature set we will use as a baseline in the next stage of our experiments is bag of word forms. The feature set recieved an F-score of $0.6123$, with a precision of $0.6777$ and a recall of $0.5585$, using the MaxEnt classifier after tuning. We will also be using only the MaxEnt classifier for the remainder of the feature set experiments.

\begin{table}
  \begin{center}
    \begin{tabular}{lrrr}
      \toprule
      
      $n$-gram combination & BoW & BoW+BoL \\
      \midrule
      no $n$-grams & $0.6123$ & $0.6278$ \\
      bigrams & $\textbf{0.6376}$ & $0.6577$ \\
      trigrams & $0.6180$ & $0.6453$ \\
      bi- and trigrams & $0.6337$ & $\textbf{0.6656}$ \\
      \bottomrule

    \end{tabular}
  \end{center}
  \caption{F-scores of the bag of words, and bag of words and lemma feature sets, combined with word form bigrams, trigrams or both.}
  \label{tab:ngrams}
  % tidligere Tabell 4
\end{table}

\begin{table}
  \begin{center}
    \begin{tabular}{lrrrrr}
      \toprule
      
     & POS\textsubscript{Lex} & Dep\textsubscript{Lex} & Synset & Brown\\ % POS er uPOS
      \midrule
      BoF & $0.6018$ & $0.5655$ & $0.4922$ & $0.4688$ \\ % Synset regular, Brown 3200
      %% BoF & $0.0018$  &  $0.0143$ & $0.4922$ & $0.4688$ \\ % Synset regular, Brown 3200
      BoW+BoF & $0.6071$ & $0.6185$ & $0.6176$ & $0.6145$\\ % Synset regular, Brown100
      
      \bottomrule

    \end{tabular}
  \end{center}
  \caption{Results of bag of feature variations with different feature types. The results shown are the F-scores of each feature set after tuning, using the MaxEnt classifier.}
  \label{tab:bagOfFeatures}
  % tidligere Tabell 3
\end{table}



We go on to test word form $n$-grams with bag of lexical features. As seen in Table \ref{tab:ngrams}, we test bag of words alone, and bag of words with bag of lemmas, combined with the word form variants of bigrams, trigrams and both. The best result without bag of lemmas comes from bag of words with only bigrams, with an F-score of $0.6376$, closely followed by bag of words with both types of $n$-grams, which got an F-score of $0.6337$. The best result overall came from the combination of bag of words, bag of lemmas, and both types of $n$-grams, which got an F-score of $0.6656$.


Next, we add the different feature types. We use the bag-of feature template, and for each feature, test both the bag of features on its own, and with bag of words. As seen in Table \ref{tab:bagOfFeatures}, none of the feature types alone result in higher F-scores than bag of words. However, all feature types (except the lexicalized POS-tags), combined with bag of words, perform better than bag of words alone. Though none of them beat bag of words and lemmas combined, with an F-score of $0.6278$.

\begin{table}
  \begin{center}
    \begin{tabular}{lrrrrrr}
      \toprule
      %% BoW, BoF \& 
      %% & Lemma & POS & Dep & Synset & Brown\\
      %% \midrule
      %% Bigram + backoff
      %%  \\ % synset reg, b3200
      %% Trigram + backoff
      %%  \\ % synset reg, b320
      %% $n$-gram + backoff 
      %% \\ % synset reg, b320

      & bigram & trigram & bi+trigram \\
      \midrule
      Lemma backoff & $0.6611$ & $0.6480$ &  $\textbf{0.6649}$ \\ 
      POS backoff & $\textbf{0.6410}$ & $0.6294$ & $0.6320$ \\
      Dep backoff & $0.6208$ & $0.6194$ & $\textbf{0.6220}$ \\
      Synset backoff & $0.6454$ & $0.6448$ &  $\textbf{0.6537}$\\
      Brown backoff & $0.6285$ &  $0.6173$  & $\textbf{0.6335}$\\
      
      %Bigram + backoff, trigram & $\textbf{0.6649}$ & $0.6398$ & $0.6309$ & $0.6192$ \\
      %Bigram, trigram + backoff & $\textbf{0.6649}$ & $0.6320$ & $\textbf{0.6368}$ & $\textbf{0.6220}$ \\
      %Bigram + backoff, trigram + backoff & $0.6645$ & $0.6316$ & $0.6268$ & $0.6127$ \\
      
      \bottomrule

    \end{tabular}
  \end{center}
  \caption{Backoff from different combinations of $n$-grams. The models also contain bag of words, and bag of features. Each backoff combination is the one that achieved the best result.}
  \label{tab:ngrambackoff}
\end{table}

Next, we combine the results of bag of features, and $n$-grams, with $n$-gram feature backoffs. In Table \ref{tab:ngrambackoff} we see that the lemma backoff consistently outperform the other feature types, but that even lemma backoff does not improve upon the results without backoff features. We test all possible backoff combinations.

\begin{table}
  \begin{center}
    \begin{tabular}{lrr}
      \toprule
      
      
      Dependency backoff & Simple & Combined\\
      \midrule
      w/o backoff & $0.6240$ & $\textbf{0.6586}$ \\
      Lemma & $0.6224$ & $0.6507$ \\
      POS & $\textbf{0.6298}$ & $0.6547$ \\ % uPOS, begge to
      Synset & $0.6234$ & $0.6516$  \\ % 1. regular og granparent (de er like), 2. grand parent
      Brown & $0.6299$ & $0.6504$ \\ % 1. Brown 320, 2. Brown 1000
      
      
      
      \bottomrule

    \end{tabular}
  \end{center}
  \caption{Dependency triples with and without feature backoff. Simple is the feature set containing bag of words and dependency triples. Combined is the feature set containing bag of words, bag of lemmas, bigrams, trigrams and dependency triples. Each row backs off to a different feature type.}
  \label{tab:depbackoff}
\end{table}

Lastly, we will test the addition of dependency triples to our models. We add dependency triples consisting of word forms, both alone, and in conjunction with feature backoffs. As with $n$-gram backoff, we test multiple variations of backoff, both head-backoff and modifier-backoff. The results reported are the backoff variant that acheived the best results. As seen in Table \ref{tab:depbackoff}, the inclusion of word form dependency triples improved upon the simplest model, with an F-score of $0.6240$, compared to $0.6123$ for bag of words alone. Dependency triples did not, however, improve upon the results achieved by our best model, as the best model with dependency triples got an F-score of $0.6586$ compared to our best result of $0.6656$.

The addition of backoff also did not improve upon our best model. Adding POS backoff to our simplest model resulted in a slightly increased F-score of $0.6298$. Adding backoff to our combined model, however, only resulted in poorer F-scores, with the best feature backoff (also POS) resulting in an F-score of $0.6547$.

Our final model (which we refer to as ``Combined'') is the model consisting of bag of word forms, bag of lemmas, and word form bigrams and trigrams, which got the best result, with an F-score of $0.6656$. In Subsection \ref{sec:heldout} we will also test that same model with the addition of trigram backoff to lemmas, which got an F-score of $0.6649$.


%% Morphosyntactic features:
%% - bag of, bigram, trigram of pos, deprel
%% - combine with baseline

%% Semantic features:
%% - bag of, bigram, trigram of Brown, WordNet
%% - combine with baseline

%% Backoff: best of morphosyntactic and semantic?

%% Combination of best from baseline, morphosyntactic and semantic

\subsection{Error analysis}

%% Before we perform our held-out testing, we will examine what types of errors we have in our model, when tested on the development set. 
The combined feature set acheived a precision of $0.7709$, and a recall of $0.5857$. Compared with our baseline system, which had a precision of $0.6777$, and a recall of $0.5585$, we see that the majority of the improvement over the baseline comes from the increase in precision. When reviewing a random subsample of the false positives and false negatives, we see that the noisy data has caused some problems for the preprocessor. Another source of errors, specifically for precision, are comments which use multiple typically threatening words in a non-threatening context.


%% \begin{table}
%%   \begin{center}
%%     \begin{tabular}{llrr}
%%       \toprule
      
%%       & & Actual & \\
%%       &  & Positive & Negative\\
%%       \midrule
%%       Predicted & Positive & $646$ & $192$ \\
%%       & Negative & $457$ & $21,663$ \\
      
      
%%       \bottomrule

%%     \end{tabular}
%%   \end{center}
%%   \caption{The true and false, positives and negatives of our best model, when classifying the development set. The numbers are the sum of each of the 10 classification, when performing 10-fold cross-validation.}
%%   \label{tab:positives}
%% \end{table}

\subsection{Held-out results}
\label{sec:heldout}

We performed our final testing on the held-out test set using the baseline feature set, the combined feature set, and the combined features set with trigram backoff using lemma features. As seen in Table \ref{tab:heldout}, both the best feature set, and the second best, from our development testing, outperformed the baseline feature set. The best feature set from development also outperformed the second best. The difference in F-scores between the baseline and the best feature set was not as large as in our development testing. However, this may be due to the fact that the baseline performed a lot better in the held-out testing compared to the development testing, with an F-score of $0.6562$. The F-scores of the combined feature sets were $0.6860$ and $0.6799$. In fact, both the combined feature sets improved upon the baseline in both precision, recall and F-scores.



\begin{table}
  \begin{center}
    \begin{tabular}{lrrr}
      \toprule
      
      & Precision & Recall & F-score\\
      \midrule
      Baseline & $0.7325$ & $0.5943$ & $0.6562$ \\
      Combined & $0.7532$ & $\textbf{0.6299}$ & $\textbf{0.6860}$  \\
      Combined+backoff & $\textbf{0.7703}$ & $0.6085$ & $0.6799$ \\
      
      
      \bottomrule

    \end{tabular}
  \end{center}
  \caption{Precision, recall and F-score for the baseline feature set, the best feature set, and the best backoff feature set, when classification is performed on the held-out test set.}
  \label{tab:heldout}
\end{table}




%% \section{Conclusion}
%% \label{sec:disc}

\bibliography{wassa2016}
\bibliographystyle{naaclhlt2016}


\end{document}
