%
% File naaclhlt2016.tex
%

\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2016}
\usepackage{times}
\usepackage{latexsym}
\usepackage{relsize}

\usepackage{verbatim}
\usepackage{booktabs}

\usepackage{tikz-qtree}
\usepackage{tikz-dependency}

\usepackage{xytree} % Dependency tree
\usepackage{fancyvrb} % Comments verbatim


%\naaclfinalcopy % Uncomment this line for the final submission
\def\naaclpaperid{***} %  Enter the naacl Paper ID here

\newcommand{\tov}{threat of violence}
\newcommand{\Tov}{Threat of violence}
\newcommand{\tovs}{threats of violence}
\newcommand{\Tovs}{Threats of violence}
\newcommand{\hugo}{Hugo Lewi Hammer}
\newcommand{\Dep}{Dependency parsing}
\newcommand{\dep}{dependency parsing}


\newcommand{\Ds}{The YouTube threat data set}
\newcommand{\ds}{the YouTube threat data set}

\let\tag=\textsc


% To expand the titlebox for more authors, uncomment
% below and set accordingly.
% \addtolength\titlebox{.5in}    


%todo: possibly a title that reflects the work better? 
\title{Detecting threats of violence in YouTube comments}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}
% If the title and author information does not fit in the area allocated,
% place \setlength\titlebox{<new height>} right after
% at the top, where <new height> can be something larger than 2.25in
\author{Author 1\\
	    XYZ Company\\
	    111 Anywhere Street\\
	    Mytown, NY 10000, USA\\
	    {\tt author1@xyz.org}
	  \And
	Author 2\\
  	ABC University\\
  	900 Main Street\\
  	Ourcity, PQ, Canada A1A 1T2\\
  {\tt author2@abc.ca}}

\date{}

\begin{document}

\maketitle

\begin{abstract}
This article investigates the effect of various types of linguistic
features (lexical, morphosyntactic and semantic) for the task of
detecting threats of violence in a data set of YouTube comments. It
further examines the use of these features in combination with
different representations of the linguistic context. Our results show that ...
\end{abstract}

\section{Introduction}
\label{sec:intro}
Threats of violence constitute an increasingly common occurrence in online
discussions. It disproportionally affects women and minorities, often
to the point of effectively eliminating them from taking part in
discussions online. Moderators of social networks operate on such a
large scale that manually reading all posts is an insurmountable
task. Methods for automatically detecting threats could therefore
potentially be very helpful, both to moderators of social networks,
and to the members of those networks.

In this article, we evaluate different types of features for the task
of detecting threats of violence in YouTube comments. We draw on both
lexical, morphosyntactic and lexical semantic information sources and
experiment with different machine learning algorithms. Our results
indicate that successful detection of threats of violence is largely
determined by lexical information.

\section{Previous work}
\label{sec:prev}
There is little previous work specifically devoted to the detection of
threats of violence in text, however, there is previous work which
examines other types of closely related phenomena, such as
cyberbullying and hate-speech.

\newcite{dinakar2011} propose a method for the detection of
cyberbullying by targeting combinations of profane or negative words,
and words related to several predetermined sensitive topics. Their
data set consists of over 50,000 YouTube comments taken from videos
about controversial topics. %% They adopt a two-stage approach, where the
%% first stage consists of using a lexicon of negative words and a list
%% of profane words, as well as part-of-speech tags from the training
%% data that were correlated with bullying. The second stage is
%% category-specific, and makes use of commonly observed uni- and bigrams
%% from each category as features.
The experiments reported accuracies from 63~\% to 80~\%, but did not
report precision or recall.
% lists of negative and profane words
% correlated words from training data

There has been quite a bit of work focused on the detection of threats
in a data set of Dutch tweets \cite{nellngram2013,nellshallow2013}, which consists of a collection of 5000
threatening tweets collected by a website. In addition, a large number of random tweets were collected for
development and testing. %% A set of 2.3 million random tweets was used
%% for development, and a set of 1 million was used for testing.
The system relies on manually constructed recognition patterns, in the
form of n-grams 
%(uni-, bi- and trigrams, as well as skip bi- and trigrams)
, but do not go into detail about the methods used to
construct these patterns%% , stating that the researchers relied on their
%% (linguistic) intuition as speakers of Dutch \cite{nellngram2013}
. In
\newcite{nellshallow2013}, a manually crafted shallow parser is added
to the system. This improves results to a precision of 39\% and a
recall of 59\%.
% handcrafted patterns
% rule-based shallow parser
% dubious evaluation

\newcite{warner2012} present a method for detecting hate speech in
user-generated web text from the internet, which relies on machine
learning in combination with template-based features. The task
is approached as a word-sense disambiguation task, since the same
words can be used in both hateful and non-hateful contexts. The
features used in the classification were combinations of uni-, bi- and
trigrams, part-of-speech-tags and Brown clusters.  The best results of
the classifications were obtained using only unigrams as features,
with a precision of 67~\% and a recall of 60~\%. The other feature
sets garnered much lower results, and the authors suggest that deeper
parsing could reveal significant phrase patterns.
% only unigrams best results
% suggests parsing might help

Perhaps closest to the current work is the work of
\newcite{hammer2014}, who reports on an experiment on the same data set that
we will be using in our own experiments. The method uses a logistic
LASSO regression analysis on bigrams (skip-grams) of important words
to classify sentences as \tovs{} or not.  The system makes use of a
list of words that are correlated with threats of violence. The
article does not, however, describe exactly how these important words
were selected, stating only that words were chosen that were
significantly correlated with the response (violent/non-violent
sentence). Results are reported only in terms of proportion of false
positives for the two classes and it is not clear how the data was
split for training and evaluation, which makes it difficult to compare with their results.


\section{The YouTube threat data set}
\label{sec:data}
The YouTube threat data set is comprised of user-written comments from
eight different YouTube videos \cite{hammer2014}. A comment
consists of a set of sentences, each of them manually annotated to be
either a \tov{} (or support for a \tov{}) or not. The data set
furthermore records the username\footnote{The usernames were annonymized by us}
%% \footnote{In 2013, YouTube changed its
%%   commenting system from using unique usernames, to using "real
%%   names", like Facebook and other sites \cite{youtube2013}. Some
%%   accounts, however, did not provide real names, so they continue to
%%   only be identified by their usernames.}
  of the user that posted the
comment. The eight videos that the comments were posted to cover religious and political topics like halal slaughter, immigration,
Anders Behring Breivik, Jihad, etc. \cite{hammer2014}. 
%The videos cover slightly different topics, the videos all contain the same
%type of discussions, namely of xenophobia and racism.

The data set consists of 9,845 comments, comprised of 28,643
sentences, see table \ref{tab:dataset}. In total there are 402,673
tokens in the sentences in the data set. There are 1,285 comments
containing threats, and 1,384 sentences containing threats, as seen in
table \ref{tab:dataset}.  \newcite{hammer2014} report inter annotator
agreement on this data set to be 98~\%, as calculated on 120 of the
comments, doubly annotated for evaluation.

\begin{table}
\begin{center}
\begin{tabular}{lrrr}
  %&   &   &   \\
  \toprule
  
  & Commments  & Sentences  & Users posting \\
  \midrule
	
  
  Total  & $9,845$  & $28,643$  & $5,483$  \\
  Threats  & $1,285$  & $1,384$  & $992$  \\
  \bottomrule

\end{tabular}
\end{center}
\caption{Number of comments, sentences and users in \ds{}}
\label{tab:dataset}
\end{table}

%to-do: should we anonymize the user names?
Figure \ref{fig:comments} provides some examples of comments containing
\tovs{} taken from the data set. The first line is the username or
name, and the subsequent lines are the sentences of the comment. An
empty line indicates the end of a comment. The sentences are annotated
with a number indicating whether they contain a \tov{} (1), or not
(0).

\begin{figure}
\begin{Verbatim}[fontsize=\small]
User #44
1    and i will kill every
     fucking muslim and arab!

User #88
0    Need a solution?
1    Drop one good ol' nuke on
     that black toilet in Mecca.

User #138
1    Funny, We will conquer you
     all in just few years, U will
     be my slave and your women
     will be my Sex Toy in Bed.
\end{Verbatim}
\caption{Examples of comments from the data set.}
\label{fig:comments}
\end{figure}

\section{Experiments}
\label{sec:exp}
Much of the previous work in Section \ref{sec:prev} made use of
precompiled lists of correlated words and manually crafted
patterns. Whereas these resources can be effective, they are highly
task-specific and do not easily lend themselves to replication.  The
previous work further highlights the effectiveness of lexical features
(words-based features), however, several of the authors suggest that parsing
may be beneficial for these tasks.

In this work we experiment with a wide range of linguistic information
as features in our system and investigate the introduction of
linguistic context through a set of feature templates. We also
generalize these features through a backoff technique. Throughout, we
make use of freely available, re-usable resources and tools in order
to construct our system.


\subsection{Experimental setup}

\paragraph{Pre-processing}Since threat annotation is performed on the sentence level, the data
set has been manually split into sentences as part of the annotation
process.  We performed tokenization, lemmatization, PoS-tagging and
dependency parsing using the SpaCy
toolkit\footnote{https://spacy.io/}.  SpaCy assigns both the standard
Penn Treebank PoS-tags \cite{Mar:San:Mar:93}, as well as the more
coarse-grained Universal PoS tag set of \newcite{Pet:Das:McD:12}. The
dependency parser assigns an analysis in compliance with the ClearNLP
converter \cite{Cho:Pal:12}, see Figure \ref{fig:dep} for an example
dependency graph from the data set. The data set was further enriched
with the cluster labels described in Turian et
al.~\shortcite{Tur:Rat:Ben:10}, created using the Brown clustering
algorithm \cite{Bro:deS:Mer:92} and induced from the RCV1 corpus, a
corpus containing Reuters English newswire text, with approximately 63
million words and 3.3 million sentences. We vary the number of
clusters to be either 100, 320, 1000 or 3200 clusters and use the full
cluster label.  We also make use of the WordNet resource
\cite{wordnet} to determine a word's synset, parent and grandparent
synset.

\begin{figure*}
\begin{center}
\begin{dependency}[arc edge, text only label, theme=simple]
  \begin{deptext}[column sep=.2cm]
    i \& m \& fucking \& scared \& kill \& them \& d \& : \\
   \tag{\smaller pron} \& \tag{\smaller verb} \& \tag{\smaller verb} \& \tag{\smaller adj} \& \tag{\smaller verb} \& \tag{\smaller pron} \& \tag{\smaller x} \& \tag{\smaller punct} \\
  \end{deptext}
  \depedge{2}{1}{\large nsubj}
  \depedge{2}{8}{\large punct}
  \depedge{2}{4}{\large acomp}
  \depedge{4}{3}{\large amod}
  \depedge{2}{5}{\large conj}
  \depedge{5}{6}{\large dobj}
  \depedge{5}{7}{\large advmod}
  \deproot[edge unit distance=1.5ex]{2}{}
\end{dependency}
\caption{Dependency parse of example sentence from the data set, with assigned uPOS tags.}
\label{fig:dep}
\end{center}
\end{figure*}



\begin{figure*}
  \begin{center}

    \xytext{
      \xybarnode{i} &~~~&
      \xybarnode{m}
      \xybarconnect(UL,U){-2}"_{\small nsubj}"
      \xybarconnect[21](UR,U){12}"^{\small punct}"
      \xybarconnect[15](UR,UL){6}"^{\small conj}"
      \xybarconnect[9](UR,U){4}"^{\small acomp}"
      &~~~&
      \xybarnode{fucking} &~~~&
      \xybarnode{scared}
      \xybarconnect(UL,U){-2}"_{\small amod}"
      &~~~&
      \xybarnode{kill}
      \xybarconnect[9](U,U){4}"^{\small advmod}"
      \xybarconnect(UR,U){2}"^{\small dobj}"
      &~~~&
      \xybarnode{them}
      &~~~&
      \xybarnode{d}
      &~~~&
      \xybarnode{:}
    }
    \caption{A visual representation of the dependency graph of our example sentence.}
    \label{fig:dependencygraph}
  \end{center}
\end{figure*}

\paragraph{Classifiers}
Toolkits and references
\paragraph{Tuning}
When tuning each model, we aimed to maximize the F-score of the model. In the MaxEnt classifier and SVM we tuned the C-value, which for both classifiers is the inverse of regularization strength. When tuning these classifiers, we started with C-values from 1 to 150 in 10-value increments, selected the best performing C-value and repeated the process with ever decreasing increments, with the range of C-values centered on the best performing C-value thus far. After 6 iterations, we terminate the tuning, and select the best performing C-value. When tuning the RandomForest classifier, we performed a grid search over the max number features used when splitting a node in the tree (sqrt($n$), log($n$) or $n$, where $n$ is the number of features in the model), and the number of trees (10, 100 or 1000, depending on the size of the feature set).

When performing tuning on the MaxEnt classifier with the bag of words features set, F-score improved from $0.5622$, using the default settings (C-value = 1), to $0.6123$, using the best C-value ($11.25$). The large increase in F-score after tuning leads us to perform tuning on every feature set we test.

\paragraph{Features}
Based on the enriched data set, as described above, we experiment with the following lexical, morphosyntactic and semantic feature types:
\begin{description}
\setlength\itemsep{0em}
\item[Lexical] word form, lemma
\item[Morphosyntactic] Penn Treebank (PTB) or Universal (uPOS) PoS and Dependency Relation
\item[Semantic] Brown cluster label (100, 320, 1000 and 3200 clusters) and WordNet synset, parent synset and grandparent synset
\end{description}
These features are structured according to a set of \emph{feature
  templates}, which introduce varying degrees of linear order and
syntactic context: bag-of-features (unordered), bigrams, trigrams and
dependency triples. These feature templates are applied to the feature
types presented above, in order to yield features like the following
for our examples sentence in Figure \ref{fig:dep}:

\begin{itemize}
\setlength\itemsep{0em}
\item \{i m,  m fucking, fucking scared, scared kill, kill them,  them d,  d :\}

\item \{PRON VERB VERB, VERB VERB ADJ, VERB ADJ VERB, ADJ VERB PRON,  VERB PRON X,  PRON X PUNCT\}

\item \{$<$m, nsubj, i$>$, $<$root, root, m$>$, $<$scared, amod, fucking$>$, $<$m, acomp, scared$>$, $<$m, conj, kill$>$, $<$kill, dobj, them$>$, $<$kill, advmod, d$>$, $<$m, punct, :$>$ \}
\end{itemize}
Our lexical features are very specific and require the exact
combination of two lexical items in order to apply to a new instance.
Following \newcite{Jos:Pen:2009}, we therefore experiment with
generalization of features by backoff to a more general category,
e.g. from word form to lemma or PoS. A dependency triple over word
forms like $<$kill, dobj, them$>$ would thus be generalized to
$<$VERB, dobj, them$>$ using \emph{head-backoff} and $<$kill, dobj,
PRON$>$ using \emph{modifier-backoff}. We apply backoff to bigram,
trigram and dependency triple features.
\subsection{Development results}
Table \ref{tab:baseline} shows baseline results in terms of F-score
for bag-of, bigram and trigram templates over lexical features (word
form and lemma). The presented results constitute the best F-scores
after tuning, as discussed above. The
overall best result came from the logistic regression classifier with
the bag of word form feature set. Generally we see that feature sets
containing bag of lexical features in some form outperform the
$n$-gram feature sets.
The feature set we will use as a baseline in the next stage of our
experiments is bag of word forms. The feature set recieved an F-score
of $0.6123$, with a precision of $0.6777$ and a recall of $0.5585$,
using the MaxEnt classifier after tuning. We will also be
using only the MaxEnt classifier for the remainder of the
feature set experiments.
\begin{table}
  \begin{center}
    \begin{tabular}{lrrr}
      \toprule

      & MaxEnt & SVM & RF \\
      \midrule
      Bag of word forms & $\textbf{0.6123}$ & $\textbf{0.6068}$ & $\textbf{0.5918}$ \\ % SVM bag of words er bare best dersom word form + uPOS ikke er med i tabellen
      Bag of lemmas & $0.5902$ & $0.5982$ & $0.5856$ \\
      Bigrams & $0.4856$ & $0.4887$ & $0.4944$ \\
      Trigrams & $0.2776$ & $0.2856$ & $0.2859$ \\
      
      
      \bottomrule
      % Data fra firstPass i classify
      % Source/printFirstPassResults.py
    \end{tabular}
  \end{center}
  \caption{Results for baseline system; F-score for bag-of, bigram and trigram templates over lexical features (word form and lemma). MaxEnt is the MaximumEntropy classifier, SVM is the support vector machine, and RF is the RandomForest classifier.}
  \label{tab:baseline}
\end{table}

\begin{table*}
  \begin{center}
    \begin{tabular}{lrrrrr}
      \toprule
      
      & Lemma & POS & Dep & Synset & Brown\\ % POS er uPOS
      \midrule
      Bag of Features & $0.5902$ & $0.0018$   &  $0.0143$ & $0.4922$ & $0.4688$ \\ % Synset regular, Brown 3200
      BoW + Bag of Features & $0.6278$ & $0.6071$ & $0.6185$ & $0.6176$ & $0.6145$\\ % Synset regular, Brown100
      
      \bottomrule

    \end{tabular}
  \end{center}
  \caption{Results of bag of feature variations with different feature types. The results shown are the F-scores of each feature set after tuning, using the MaxEnt classifier.}
  \label{tab:bagOfFeatures}
\end{table*}


Next, we add the different feature types. We first use the bag of features feature template, for each feature, both on its own, and in a feature set with bag of word forms. As seen in table \ref{tab:bagOfFeatures}, none of the feature types alone result in higher F-scores than bag of words. However, all feature types but the POS, combined with bag of words, perform better than bag of words alone, with the combination of bag of lemmas and bag of words performing best, with an F-score of $0.6278$.

\begin{table}
  \begin{center}
    \begin{tabular}{lrrrr}
      \toprule
      
      & F-score \\
      \midrule
      Bag of words \& bigrams & $0.6376$ \\
      Bag of words \& trigrams & $0.6180$ \\
      Bag of words \& $n$-grams & $0.6337$ \\
      \bottomrule

    \end{tabular}
  \end{center}
  \caption{F-scores of the bag of words feature sets, combined with word form bigrams, trigrams or both (termed simply $n$-grams in the table).}
  \label{tab:ngrams}
\end{table}

Now we test word form $n$-grams with bag of words. As seen in table \ref{tab:ngrams}, we test bag of words combined with the word form variants of bigrams, trigrams and both. The best result comes from bag of words with only bigrams, with an F-score of $0.6376$, closely followed by bag of words with both types of $n$-grams, which got an F-score of $0.6337$. When we combine the $n$-grams combinations with both bag of words and bag of features, we get a new best result from bag of words, bag of lemmas and both types of $n$-grams, which gets an F-score of $0.6656$.

\begin{table*}
  \begin{center}
    \begin{tabular}{lrrrrrr}
      \toprule
      BoW, BoF \& & Lemma & POS & Dep & Synset & Brown\\
      \midrule
      Bigram + backoff & $0.6611$ & $0.6410$  & $0.6208$ & $0.6454$ & $0.6285$\\ % synset reg, b3200
      Trigram + backoff & $0.6480$ & $0.6294$ & $0.6194$ & $0.6448$ & $0.6173$ \\ % synset reg, b320
      $n$-gram + backoff & $0.6649$ & $0.6320$ & $0.6220$ & $0.6537$ & $0.6335$\\ % synset reg, b320
      %Bigram + backoff, trigram & $\textbf{0.6649}$ & $0.6398$ & $0.6309$ & $0.6192$ \\
      %Bigram, trigram + backoff & $\textbf{0.6649}$ & $0.6320$ & $\textbf{0.6368}$ & $\textbf{0.6220}$ \\
      %Bigram + backoff, trigram + backoff & $0.6645$ & $0.6316$ & $0.6268$ & $0.6127$ \\
      
      \bottomrule

    \end{tabular}
  \end{center}
  \caption{Bag of words, bag of features and different combinations of $n$-grams with feature backoff. Each backoff combination is the one that achieved the best result.}
  \label{tab:ngrambackoff}
\end{table*}

Next we combine the results of bag of features, and $n$-grams, with $n$-gram feature backoffs. In table \ref{tab:ngrambackoff} we see that the lemma backoff consistently outperform the other feature types, but that even lemma backoff does not improve upon the result of including backoff features.


\begin{table*}
  \begin{center}
    \begin{tabular}{lrrrrrr}
      \toprule
      
      & w/o backoff & Lemma & POS & Tag & Synset & Brown\\
      \midrule
      Dependency triples w/ backoff  BoW & $0.6240$ &  $0.6224$ & $0.6298$ & $0.6135$ & $0.6234$ & $0.6299$  \\ % Synset regular og grand parent, Brown 320
      Dependency triples w/ backoff BoW, BoL \& n-grams & $0.6586$ & $0.6507$ & $0.6547$ & $0.6536$ & $0.6516$ & $0.6504$\\ % Synset gp, Brown 1000
      
      \bottomrule

    \end{tabular}
  \end{center}
  \caption{F-scores of the different feature backoffs from dependency triples.}
  \label{tab:firsttriplebackoff}
\end{table*}

\begin{table}
  \begin{center}
    \begin{tabular}{lrr}
      \toprule
      
      
      & BoW & Best\\
      \midrule
      w/o backoff & $0.6240$ & $0.6586$ \\
      Lemma & $0.6224$ & $0.6507$ \\
      POS & $0.6298$ & $0.6547$ \\ % uPOS, begge to
      Dep \\
      Synset & $0.6234$ & $0.6516$  \\ % 1. regular og granparent (de er like), 2. grand parent
      Brown & $0.6299$ & $0.6504$ \\ % 1. Brown 320, 2. Brown 1000
      
      
      
      \bottomrule

    \end{tabular}
  \end{center}
  \caption{BoW is BoW \& dep triples, Best is BoW, BoL, n-grams \& dep triples}
  \label{tab:firsttriplebackoff}
\end{table}

Add dependency triples, add dependency backoff, no improvement. Construct final model, no improvement.


%% Morphosyntactic features:
%% - bag of, bigram, trigram of pos, deprel
%% - combine with baseline

%% Semantic features:
%% - bag of, bigram, trigram of Brown, WordNet
%% - combine with baseline

%% Backoff: best of morphosyntactic and semantic?

%% Combination of best from baseline, morphosyntactic and semantic

Held-out

\subsection{Error analysis}

\subsection{Held-out results}

\section{Conclusion}
\label{sec:disc}

\bibliography{wassa2016}
\bibliographystyle{naaclhlt2016}


\end{document}
