%
% File naaclhlt2016.tex
%

\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2016}
\usepackage{times}
\usepackage{latexsym}
\usepackage{relsize}

\usepackage{verbatim}
\usepackage{booktabs}

\usepackage{tikz-qtree}
\usepackage{tikz-dependency}


%\naaclfinalcopy % Uncomment this line for the final submission
\def\naaclpaperid{***} %  Enter the naacl Paper ID here

\newcommand{\tov}{threat of violence}
\newcommand{\Tov}{Threat of violence}
\newcommand{\tovs}{threats of violence}
\newcommand{\Tovs}{Threats of violence}
\newcommand{\hugo}{Hugo Lewi Hammer}
\newcommand{\Dep}{Dependency parsing}
\newcommand{\dep}{dependency parsing}


\newcommand{\Ds}{The YouTube threat data set}
\newcommand{\ds}{the YouTube threat data set}

\let\tag=\textsc


% To expand the titlebox for more authors, uncomment
% below and set accordingly.
% \addtolength\titlebox{.5in}    


%todo: possibly a title that reflects the work better? 
\title{Detecting threats of violence in YouTube comments}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}
% If the title and author information does not fit in the area allocated,
% place \setlength\titlebox{<new height>} right after
% at the top, where <new height> can be something larger than 2.25in
\author{Author 1\\
	    XYZ Company\\
	    111 Anywhere Street\\
	    Mytown, NY 10000, USA\\
	    {\tt author1@xyz.org}
	  \And
	Author 2\\
  	ABC University\\
  	900 Main Street\\
  	Ourcity, PQ, Canada A1A 1T2\\
  {\tt author2@abc.ca}}

\date{}

\begin{document}

\maketitle

\begin{abstract}
Here comes an abstract for this article
\end{abstract}

\section{Introduction}
\label{sec:intro}
Threats of violence constitute an increasingly common occurrence in online
discussions. It disproportionally affects women and minorities, often
to the point of effectively eliminating them from taking part in
discussions online. Moderators of social networks operate on such a
large scale that manually reading all posts is an insurmountable
task. Methods for automatically detecting threats could therefore
potentially be very helpful, both to moderators of social networks,
and to the members of those networks.

In this article, we evaluate different types of features for the task
of detecting threats of violence in YouTube comments. We draw on both
lexical, morphosyntactic and lexical semantic information sources and
experiment with different machine learning algorithms. Our results
indicate that successful detection of threats of violence is largely
determined by lexical information.

\section{Previous work}
\label{sec:prev}
There is little previous work specifically devoted to the detection of
threats of violence in text, however, there is previous work which
examines other types of closely related phenomena, such as
cyberbullying and hate-speech.

\newcite{dinakar2011} proposes a method for the detection of
cyberbullying by targeting combinations of profane or negative words,
and words related to several predetermined sensitive topics. Their
data set consists of over 50,000 YouTube comments taken from videos
about controversial topics. They adopt a two-stage approach, where the
first stage consists of using a lexicon of negative words and a list
of profane words, as well as part-of-speech tags from the training
data that were correlated with bullying. The second stage is
category-specific, and makes use of commonly observed uni- and bigrams
from each category as features. The experiments reported accuracies
from 63~\% to 80~\%, but did not report precision or recall.
% lists of negative and profane words
% correlated words from training data

There has been quite a bit of work focused on the detection of threats
in a data set of Dutch tweets \cite{nellngram2013,nellshallow2013}.
The data set used for these experiments consists of a collection of 5000
threatening tweets collected by a website over a period of about two
years. In addition, a large number of random tweets were collected for
development and testing. A set of 2.3 million random tweets was used
for development, and a set of 1 million was used for testing. 
The system relies on manually constructed recognition patterns, in the
form of n-grams (uni-, bi- and trigrams, as well as skip bi- and
trigrams), but do not go into detail about the methods used to
construct these patterns, stating that the researchers relied on their
(linguistic) intuition as speakers of Dutch \cite{nellngram2013}. In
\newcite{nellshallow2013}, a manually crafted shallow parser is added
to the system. This improves results to a precision of 39\% and a
recall of 59\% \footnote{Perhaps a footnote on the peculiarities of the evaluation??}.
% handcrafted patterns
% rule-based shallow parser
% dubious evaluation

\newcite{warner2012} present a method for detecting unwanted or
illegal comments in user-generated web text from the internet, which
relies on machine learning in combination with template-based
features. The data set used in the research came from two sources. The
first consists of posts from Yahoo news groups, the second were
webpages collected by the American Jewish Congress that had been
identified as offensive. The data set was manually annotated, and then
the hate speech was assigned to a category, such as anti-Semitic,
anti-woman, anti-Asian, etc. The task is then approached as a
word-sense disambiguation task, since the same words can be used in
both hateful and non-hateful contexts. The features used in the
classification were combinations of uni-, bi- and trigrams,
part-of-speech-tags and Brown clusters.  The best results of the
classifications were obtained using only unigrams as features, with a
precision of 67~\% and a recall of 60~\%. The other feature sets
garnered much lower results, and the authors suggest that deeper
parsing could reveal significant phrase patterns.
% only unigrams best results
% suggests parsing might help

\newcite{hammer2014} reports an experiment on the data set that we
will be using in our own experiments, although it has been changed
slightly since the publication of his initial study. The method used a
logistic LASSO regression analysis on bigrams (skip-grams) of
important words to classify sentences as \tovs{} or not.
The method described in the article uses a set of important words that
are correlated with threats of violence. The features are bigrams of
two of these important words observed in the same sentence. The
article does not describe exactly how these important words were
selected, stating only that words were chosen that were significantly
correlated with the response (violent/non-violent sentence). Results are reported only in terms of proportion of false positives for the two classes and it is not clear how the data was split for training and evaluation.


\section{The YouTube threat data set}
\label{sec:data}
The YouTube threat data set is comprised of user-written comments from
eight different YouTube videos \cite{hammer2014}. A comment
consists of a set of sentences, each of them manually annotated to be
either a \tov{} (or support for a \tov{}) or not. The data set
furthermore records the username\footnote{In 2013, YouTube changed its
  commenting system from using unique usernames, to using "real
  names", like Facebook and other sites \cite{youtube2013}. Some
  accounts, however, did not provide real names, so they continue to
  only be identified by their usernames.} of the user that posted the
comment. The eight videos that the comments were posted to cover religious and political topics like halal slaughter, immigration,
Anders Behring Breivik, Jihad, etc. \cite{hammer2014}. 
%The videos cover slightly different topics, the videos all contain the same
%type of discussions, namely of xenophobia and racism.

The data set consists of 9,845 comments, comprised of 28,643
sentences, see table \ref{tab:dataset}. In total there are 402,673
tokens in the sentences in the data set. There are 1,285 comments
containing threats, and 1,384 sentences containing threats, as seen in
table \ref{tab:dataset}.  \newcite{hammer2014} report inter annotator
agreement on this data set to be 98~\%, as calculated on 120 of the
comments, doubly annotated for evaluation.

\begin{table}
\begin{center}
\begin{tabular}{lrrr}
  %&   &   &   \\
  \toprule
  
  & Commments  & Sentences  & Users posting \\
  \midrule
	
  
  Total  & $9,845$  & $28,643$  & $5,483$  \\
  Threats  & $1,285$  & $1,384$  & $992$  \\
  \bottomrule

\end{tabular}
\end{center}
\caption{Number of comments, sentences and users in \ds{}}
\label{tab:dataset}
\end{table}

%to-do: should we anonymize the user names?
Figure \ref{fig:comments} provides some examples of comments containing
\tovs{} taken from the data set. The first line is the username or
name, and the subsequent lines are the sentences of the comment. An
empty line indicates the end of a comment. The sentences are annotated
with a number indicating whether they contain a \tov{} (1), or not
(0).

\begin{figure*}
\begin{verbatim}
timpa666
1    and i will kill every fucking muslim and arab!

NimsXdimensions
0    Need a solution?
1    Drop one good ol' nuke on that black toilet in Mecca.

Ammar Alozaibi
1    Funny, We will conquer you all in just few years, U will be
     my slave and your women will be my Sex Toy in Bed.

\end{verbatim}
\caption{Examples of comments from the data set.}
\label{fig:comments}
\end{figure*}

\section{Experiments}
\label{sec:exp}
Much of the previous work in Section \ref{sec:prev} made use of
precompiled lists of correlated words and manually crafted
patterns. Whereas these resources can be effective, they are highly
task-specific and do not easily lend themselves to replication.  The
previous work further highlights the effectiveness of lexical features
(words-based features), however, several of the authors suggest that parsing
may be beneficial for these tasks.

In this work we experiment with a wide range of linguistic information
as features in our system and investigate the introduction of
linguistic context through a set of feature templates. We also
generalize these features through a backoff technique. Throughout, we
make use of freely available, re-usable resources and tools in order
to construct our system.


\subsection{Experimental setup}

\paragraph{Pre-processing}Since threat annotation is performed on the sentence level, the data
set has been manually split into sentences as part of the annotation
process.  We performed tokenization, lemmatization, PoS-tagging and
dependency parsing using the SpaCy
toolkit\footnote{https://spacy.io/}.  SpaCy assigns both the standard
Penn Treebank PoS-tags \cite{Mar:San:Mar:93}, as well as the more
coarse-grained Universal PoS tag set of \newcite{Pet:Das:McD:12}. The
dependency parser assigns an analysis in compliance with the ClearNLP
converter \cite{Cho:Pal:12}, see Figure \ref{fig:dep} for an example
dependency graph from the data set. The data set was further enriched
with the cluster labels described in Turian et
al.~\shortcite{Tur:Rat:Ben:10}, created using the Brown clustering
algorithm \cite{Bro:deS:Mer:92} and induced from the RCV1 corpus, a
corpus containing Reuters English newswire text, with approximately 63
million words and 3.3 million sentences. We vary the number of
clusters to be either 100, 320, 1000 or 3200 clusters and use the full
cluster label.  We also make use of the WordNet resource
\cite{wordnet} to determine a word's synset, parent and grandparent
synset.

\begin{figure*}
\begin{center}
\begin{dependency}[arc edge, text only label, theme=simple]
  \begin{deptext}[column sep=.2cm]
    i \& m \& fucking \& scared \& kill \& them \& d \& : \\
   \tag{\smaller pron} \& \tag{\smaller verb} \& \tag{\smaller verb} \& \tag{\smaller adj} \& \tag{\smaller verb} \& \tag{\smaller pron} \& \tag{\smaller x} \& \tag{\smaller punct} \\
  \end{deptext}
  \depedge{2}{1}{\large nsubj}
  \depedge{2}{8}{\large punct}
  \depedge{2}{4}{\large acomp}
  \depedge{4}{3}{\large amod}
  \depedge{2}{5}{\large conj}
  \depedge{5}{6}{\large dobj}
  \depedge{5}{7}{\large advmod}
  \deproot[edge unit distance=1.5ex]{2}{}
\end{dependency}
\caption{Dependency parse of example sentence from the data set, with assigned uPOS tags.}
\label{fig:dep}
\end{center}
\end{figure*}

\paragraph{Classifiers}
Toolkits and references
\paragraph{Tuning}
Short on tuning
\paragraph{Features}
Based on the enriched data set, as described above, we experiment with the following lexical, morphosyntactic and semantic feature types:
\begin{description}
\setlength\itemsep{0em}
\item[Lexical] word form, lemma
\item[Morphosyntactic] Penn Treebank (PTB) or Universal (uPOS) PoS and Dependency Relation
\item[Semantic] Brown cluster label (100, 320, 1000 and 3200 clusters) and WordNet synset, parent synset and grandparent synset
\end{description}
These features are structured according to a set of \emph{feature
  templates}, which introduce varying degrees of linear order and
syntactic context: bag-of-features (unordered), bigrams, trigrams and
dependency triples. These feature templates are applied to the feature
types presented above, in order to yield features like the following
for our examples sentence in Figure \ref{fig:dep}:

\begin{itemize}
\setlength\itemsep{0em}
\item \{i m,  m fucking, fucking scared, scared kill, kill them,  them d,  d :\}

\item \{PRON VERB VERB, VERB VERB ADJ, VERB ADJ VERB, ADJ VERB PRON,  VERB PRON X,  PRON X PUNCT\}

\item \{$<$m, nsubj, i$>$, $<$root, root, m$>$, $<$scared, amod, fucking$>$, $<$m, acomp, scared$>$, $<$m, conj, kill$>$, $<$kill, dobj, them$>$, $<$kill, advmod, d$>$, $<$m, punct, :$>$ \}
\end{itemize}
Our lexical features are very specific and require the exact
combination of two lexical items in order to apply to a new instance.
Following \newcite{Jos:Pen:2009}, we therefore experiment with
generalization of features by backoff to a more general category,
e.g. from word form to lemma or PoS. A dependency triple over word
forms like $<$kill, dobj, them$>$ would thus be generalized to
$<$VERB, dobj, them$>$ using \emph{head-backoff} and $<$kill, dobj,
PRON$>$ using \emph{modifier-backoff}. We apply backoff to bigram,
trigram and dependency triple features.
\subsection{Development results}
Table \ref{tab:baseline} shows baseline results in terms of F-score
for bag-of, bigram and trigram templates over lexical features (word
form and lemma). The presented results constitute the best F-scores
after tuning (which was only done on the MaxEnt classifier and the
SVM, not on the random forest classifier), as discussed above. The
overall best result came from the logistic regression classifier with
the bag of word form feature set. Generally we see that feature sets
containing bag of lexical features in some form outperform the
$n$-gram feature sets.
The feature set we will use as a baseline in the next stage of our
experiments is bag of word forms. The feature set recieved an F-score
of $0.6123$, with a precision of $0.6777$ and a recall of $0.5585$,
using the MaxEnt classifier after tuning. We will also be
using only the MaxEnt classifier for the remainder of the
feature set experiments.
\begin{table*}
  \begin{center}
    \begin{tabular}{lrrr}
      \toprule

      & MaxEnt & SVM & Random Forest \\
      \midrule
      Bag of word forms & $\textbf{0.6123}$ & $\textbf{0.6068}$ & $\textbf{0.5918}$ \\ % SVM bag of words er bare best dersom word form + uPOS ikke er med i tabellen
      Bag of lemmas & $0.5902$ & $0.5982$ & $0.5856$ \\
      Bigrams & $0.4856$ & $0.4887$ & $0.4944$ \\
      Trigrams & $0.2776$ & $0.2856$ & $0.2859$ \\
      
      
      \bottomrule
      % Data fra firstPass i classify
      % Source/printFirstPassResults.py
    \end{tabular}
  \end{center}
  \caption{Results for baseline system; F-score for bag-of, bigram and trigram templates over lexical features (word form and lemma).}
  \label{tab:baseline}
\end{table*}

Morphosyntactic features:
- bag of, bigram, trigram of pos, deprel
- combine with baseline

Semantic features:
- bag of, bigram, trigram of Brown, WordNet
- combine with baseline

Combination of best from baseline, morphosyntactic and semantic

\subsection{Error analysis}

\subsection{Held-out results}

\section{Conclusion}
\label{sec:disc}

\bibliography{wassa2016}
\bibliographystyle{naaclhlt2016}


\end{document}
