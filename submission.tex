%
% File naaclhlt2016.tex
%

\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2016}
\usepackage{times}
\usepackage{latexsym}

\usepackage{verbatim}
\usepackage{booktabs}

\usepackage{pgfplots}
% Anbefalt her: https://www.sharelatex.com/learn/Pgfplots_package#Introduction
\pgfplotsset{width=10cm}

%\naaclfinalcopy % Uncomment this line for the final submission
\def\naaclpaperid{***} %  Enter the naacl Paper ID here

\newcommand{\tov}{threat of violence}
\newcommand{\Tov}{Threat of violence}
\newcommand{\tovs}{threats of violence}
\newcommand{\Tovs}{Threats of violence}
\newcommand{\hugo}{Hugo Lewi Hammer}
\newcommand{\Dep}{Dependency parsing}
\newcommand{\dep}{dependency parsing}


\newcommand{\Ds}{The YouTube threat data set}
\newcommand{\ds}{the YouTube threat data set}

% To expand the titlebox for more authors, uncomment
% below and set accordingly.
% \addtolength\titlebox{.5in}    


%todo: possibly a title that reflects the work better? 
\title{Detecting threats of violence in YouTube comments}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}
% If the title and author information does not fit in the area allocated,
% place \setlength\titlebox{<new height>} right after
% at the top, where <new height> can be something larger than 2.25in
\author{Author 1\\
	    XYZ Company\\
	    111 Anywhere Street\\
	    Mytown, NY 10000, USA\\
	    {\tt author1@xyz.org}
	  \And
	Author 2\\
  	ABC University\\
  	900 Main Street\\
  	Ourcity, PQ, Canada A1A 1T2\\
  {\tt author2@abc.ca}}

\date{}

\begin{document}

\maketitle

\begin{abstract}
Here comes an abstract for this article
\end{abstract}

\section{Introduction}
\label{sec:intro}
Threats of violence constitute an increasingly common occurrence in online
discussions. It disproportionally affects women and minorities, often
to the point of effectively eliminating them from taking part in
discussions online. Moderators of social networks operate on such a
large scale that manually reading all posts is an insurmountable
task. Methods for automatically detecting threats could therefore
potentially be very helpful, both to moderators of social networks,
and to the members of those networks.

In this article, we evaluate different types of features for the task
of detecting threats of violence in YouTube comments. We show that ...

\section{Previous work}
\label{sec:prev}
There is little previous work specifically devoted to the detection of
threats of violence in text, however, there is previous work which
examines other types of closely related phenomena, such as
cyberbullying and hate-speech.

\newcite{dinakar2011} proposes a method for the detection of
cyberbullying by targeting combinations of profane or negative words,
and words related to several predetermined sensitive topics. Their
data set consists of over 50,000 YouTube comments taken from videos
about controversial topics. The topics included sexuality, race,
culture and intelligence. The comments were grouped by topic, and then
12~\% of these were manually annotated to check that they were placed
in the right category.
The first stage of the detection was the same across all
categories. It consisted of using a lexicon of negative words and a
list of profane words, as well as part-of-speech tags from the
training data that were correlated with bullying. The second stage was
category-specific, and used commonly observed uni- and bigrams from
each category as features. The experiments reported accuracies from
63~\% to 80~\%, but did not report precision or recall.

There has been quite a bit of work focused on the detection of threats
in a data set of Dutch tweets \cite{nellngram2013,nellshallow2013}.
The data set used for these experiments consisted a collection of 5000
threatening tweets collected by a website over a period of about two
years. In addition, a large number of random tweets were collected for
development and testing. A set of 2.3 million random tweets was used
for development, and a set of 1 million was used for testing. This set
was not annotated in any way prior to being used in the experiments.
The system relies on manually constructed recognition patterns, in the
form of n-grams (uni-, bi- and trigrams, as well as skip bi- and
trigrams), but do not go into detail about the methods used to
construct these patterns, stating that the researchers relied on their
(linguistic) intuition as speakers of Dutch \cite{nellngram2013}. In
\newcite{nellshallow2013}, a manually crafted shallow parser is added
to the system. This improves results to a precision of 39\% and a
recall of 59\%.

\newcite{warner2012} present a method for detecting unwanted or
illegal comments in user-generated web text from the internet, which
relies on machine learning in combination with template-based
features. The data set used in the research came from two sources. The
first consists of posts from Yahoo news groups, the second were
webpages collected by the American Jewish Congress that had been
identified as offensive. The data set was manually annotated, and then
the hate speech was assigned to a category, such as antisemitic,
anti-woman, anti-asian, etc. The research then focused on the
antisemitic category.  The research approaches the problem as a
word-sense disambiguation task, since the same words can be used in
both hateful and non-hateful contexts. The features used in the
classification were combinations of uni-, bi- and trigrams,
part-of-speech-tags and Brown clusters.  The best results of the
classifications were obtained using only unigrams as features, with a
precision of 67~\% and a recall of 60~\%. The other feature sets
garnered much lower results, and the authors suggest that deeper
parsing could reveal significant phrase patterns.

\section{The YouTube threat data set}
\label{sec:data}
The YouTube threat data set is comprised of user-written comments from
eight different YouTube videos \cite{hammer2014}. A comment
consists of a set of sentences, each of them manually annotated to be
either a \tov{} (or support for a \tov{}) or not. The data set
furthermore records the username\footnote{In 2013, YouTube changed its
  commenting system from using unique usernames, to using "real
  names", like Facebook and other sites \cite{youtube2013}. Some
  accounts, however, did not provide real names, so they continue to
  only be identified by their usernames.} of the user that posted the
comment. The eight videos that the comments were posted to cover religious and political topics like halal slaughter, immigration,
Anders Behring Breivik, Jihad, etc. \cite{hammer2014}. 
%The videos cover slightly different topics, the videos all contain the same
%type of discussions, namely of xenophobia and racism.

The data set consists of 9,845 comments, comprised of 28,643
sentences, see table \ref{tab:dataset}. In total there are 402,673
tokens in the sentences in the data set. There are 1,285 comments
containing threats, and 1,384 sentences containing threats, as seen in
table \ref{tab:dataset}.  \newcite{hammer2014} report inter annotator
agreement on this data set to be 98~\%, as calculated on 120 of the
comments, doubly annotated for evaluation.

\begin{table}
\begin{center}
\begin{tabular}{lrrr}
  %&   &   &   \\
  \toprule
  
  & Commments  & Sentences  & Users posting \\
  \midrule
	
  
  Total  & $9,845$  & $28,643$  & $5,483$  \\
  Threats  & $1,285$  & $1,384$  & $992$  \\
  \bottomrule

\end{tabular}
\end{center}
\caption{Number of comments, sentences and users in \ds{}}
\label{tab:dataset}
\end{table}

%to-do: should we anonymize the user names?
Figure \ref{fig:comments} provides some examples of comments containing
\tovs{} taken from the data set. The first line is the username or
name, and the subsequent lines are the sentences of the comment. An
empty line indicates the end of a comment. The sentences are annotated
with a number indicating whether they contain a \tov{} (1), or not
(0).

\begin{figure*}
\begin{verbatim}
timpa666
1    and i will kill every fucking muslim and arab!

NimsXdimensions
0    Need a solution?
1    Drop one good ol' nuke on that black toilet in Mecca.

Ammar Alozaibi
1    Funny, We will conquer you all in just few years, U will be
     my slave and your women will be my Sex Toy in Bed.

\end{verbatim}
\caption{Examples of comments from the data set.}
\label{fig:comments}
\end{figure*}

\section{Experiments}
\label{sec:exp}

\subsection{Experimental setup}
The data set is split into an 80-20 training and testing set. All
tuning and development experiments were performed using 10-fold
cross-validation on the training set.

Since threat annotation is performed on the sentence level, the data
set has been manually split into sentences as part of the annotation
process.  We performed tokenization, lemmatization, PoS-tagging and
dependency parsing using the spaCy toolkit REF.  SpaCy assigns both
the standard Penn Treebank PoS-tags REF, as well as the more
coarse-grained Universal PoS tag set of \newcite{Pet:Das:McD:12}. The
dependency parser assigns an analysis in compliance with the ClearNLP
converter REF. The data set was further enriched with the cluster
labels described in Turian et al.~\shortcite{Tur:Rat:Ben:10}, created
using the Brown clustering algorithm \cite{Bro:deS:Mer:92} and induced
from the RCV1 corpus, a corpus containing Reuters English newswire
text, with approximately 63 million words and 3.3 million sentences.
WordNet REF



%% todo:
%% - data split
%% - preprocessing
%%   Spacy, Brown clusters, WordNet
%% - machine learner
%% - tuning
%% - feature templates

\subsection{Results}

\subsection{Error analysis}

\subsection{Held-out results}

\section{Conclusion}
\label{sec:disc}

\bibliography{wassa2016}
\bibliographystyle{naaclhlt2016}


\end{document}
